{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3교시 데이터 타입\n",
    "\n",
    "> 스파크에서 사용되는 데이터 타입에 대해 실습합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 리터럴 타입](#1.-리터럴-타입)\n",
    "* [2. 불리언 형 데이터 타입 다루기](#2.-불리언-형-데이터-타입-다루기)\n",
    "* [3. 수치형 데이터 타입 다루기](#3.-수치형-데이터-타입-다루기)\n",
    "* [4. 문자열 데이터 타입 다루기](#4.-문자열-데이터-타입-다루기)\n",
    "* [5. 정규 표현식](#5.-정규-표현식)\n",
    "* [6. 날짜와 타임스팸프 데이터 타입 다루기](#6.-날짜와-타임스팸프-데이터-타입-다루기)\n",
    "* [7. 널 값 다루기](#7.-널-값-다루기)\n",
    "* [참고자료](#참고자료)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://52858233d06f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9422961dc0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" DataFrame 생성 \"\"\"\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"retail\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 리터럴 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>5</th><th>five</th><th>5.0</th></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+----+---+\n",
       "|  5|five|5.0|\n",
       "+---+----+---+\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "+---+----+---+"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 불리언 형 데이터 타입 다루기\n",
    "### 2.1 AND 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536365|WHITE HANGING HEA...|\n",
      "|   536365| WHITE METAL LANTERN|\n",
      "|   536365|CREAM CUPID HEART...|\n",
      "|   536365|KNITTED UNION FLA...|\n",
      "|   536365|RED WOOLLY HOTTIE...|\n",
      "|   536365|SET 7 BABUSHKA NE...|\n",
      "|   536365|GLASS STAR FROSTE...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "x1 = df.where(col(\"InvoiceNO\") != 536365).select(\"InvoiceNO\", \"Description\")\n",
    "x2 = df.where(\"InvoiceNO <> 536365\").select(\"InvoiceNO\", \"Description\") #일치하지 않음\n",
    "x3 = df.where(\"InvoiceNO = 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "\n",
    "x1.show(2)\n",
    "x2.show(2)\n",
    "x3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 OR 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536370|     POST|       POSTAGE|       3|2010-12-01 08:45:00|     18.0|   12583.0|        France|\n",
      "|   536403|     POST|       POSTAGE|       1|2010-12-01 11:27:00|     15.0|   12791.0|   Netherlands|\n",
      "|   536527|     POST|       POSTAGE|       1|2010-12-01 13:04:00|     18.0|   12662.0|       Germany|\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "df.where(\"UnitPrice > 600 OR instr(Description, 'POSTAGE') >= 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ISIN - 제공된 목록에 포함되었는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|     POST|\n",
      "|       C2|\n",
      "|      DOT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL 을 이용한 is in 구문 사용\n",
    "from pyspark.sql.functions import desc\n",
    "df.select('StockCode').where(\"StockCode in ('DOT', 'POST', 'C2')\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 INSTR - 특정 문자열이 포함되었는지\n",
    "- 주어진 문자열에서 substr 열이 처음 나타나는 위치를 찾습니다. 인수 중 하나가 null이면 null을 반환합니다.\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.instr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|added|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|    8|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|    8|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\"\"\" instr 함수 \"\"\"\n",
    "df.withColumn(\"added\", instr(df.Description, \"POSTAGE\")).where(\"added > 1\").show() # 8번째 글자에 'POSTAGE'가 시작됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 송장번호(InvoiceNo) 가 '536365' 이면서\n",
    "#### 4. 상품코드(StockCode) 가 ('85123A', '84406B', '84029G', '84029E') 중에 하나이면서\n",
    "#### 5. 제품단가(UnitPrice) 가 2.6 이하 혹은 3.0 이상인 경우를 출력하세요\n",
    "\n",
    "<details><summary>[실습1] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df1.printSchema()\n",
    "df1.show(10)\n",
    "answer = df1.where(\"InvoiceNo = '536365'\").where(\"StockCode in ('85123A', '84406B', '84029G', '84029E')\").where(\"UnitPrice < 2.6 or UnitPrice > 3.0\")\n",
    "answer.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df1.printSchema()\n",
    "df1.show(10)\n",
    "answer = df1.where(\"InvoiceNo = '536365'\").where(\"StockCode in ('85123A', '84406B', '84029G', '84029E')\").where(\"UnitPrice < 2.6 or UnitPrice > 3.0\")\n",
    "answer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 수치형 데이터 타입 다루기\n",
    "### 3.1 각종 함수를 표현식으로 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "df.selectExpr(\"CustomerID\", \"pow(Quantity * UnitPrice, 2) + 5 as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 지수만큼 제곱하는 pow 함수를 API를 사용해도 결과는 동일합니다 \"\"\"\n",
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "# 아래의 연산이 필요한 경우에는 반드시 column 으로 지정되어야 연산자 계산이 됩니다. (문자열 * 연산자는 없습니다)\n",
    "fabricateQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerID\"), fabricateQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 반올림(round), 올림(ceil), 버림(floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+\n",
      "|round(2.5, 0)|CEIL(2.4)|FLOOR(2.6)|\n",
      "+-------------+---------+----------+\n",
      "|            3|        3|         2|\n",
      "+-------------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.selectExpr(\"round(2.5, 0)\", \"ceil(2.4)\", \"floor(2.6)\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 요약 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|        InvoiceNo|\n",
      "+-------+-----------------+\n",
      "|  count|             3108|\n",
      "|   mean| 536516.684944841|\n",
      "| stddev|72.89447869788873|\n",
      "|    min|           536365|\n",
      "|    max|          C536548|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()\n",
    "df.describe(\"InvoiceNo\").show() # 컬럼을 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>2. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 송장번호(InvoiceNo) 가 '536367' 인 거래 내역의\n",
    "#### 4. 총 금액 (TotalPrice) = 수량(Quantity) * 단가(UnitPrice) 를 계산하여 TotalPrice 컬럼을 추가하세요\n",
    "#### 5. 단, 총 금액 (TotalPrice) 계산시에 소수점 이하는 버림으로 처리하세요\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df2.printSchema()\n",
    "df2.show(10)\n",
    "answer = df2.where(\"InvoiceNo = '536367'\").withColumn(\"TotalPrice\", expr(\"floor(Quantity * UnitPrice)\"))\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th><th>TotalPrice</th></tr>\n",
       "<tr><td>536367</td><td>84879</td><td>ASSORTED COLOUR BIRD ORNAMENT</td><td>32</td><td>2010-12-01 08:34:00</td><td>1.69</td><td>13047.0</td><td>United Kingdom</td><td>54</td></tr>\n",
       "<tr><td>536367</td><td>22745</td><td>POPPY&#x27;S PLAYHOUSE BEDROOM </td><td>6</td><td>2010-12-01 08:34:00</td><td>2.1</td><td>13047.0</td><td>United Kingdom</td><td>12</td></tr>\n",
       "<tr><td>536367</td><td>22748</td><td>POPPY&#x27;S PLAYHOUSE KITCHEN</td><td>6</td><td>2010-12-01 08:34:00</td><td>2.1</td><td>13047.0</td><td>United Kingdom</td><td>12</td></tr>\n",
       "<tr><td>536367</td><td>22749</td><td>FELTCRAFT PRINCESS CHARLOTTE DOLL</td><td>8</td><td>2010-12-01 08:34:00</td><td>3.75</td><td>13047.0</td><td>United Kingdom</td><td>30</td></tr>\n",
       "<tr><td>536367</td><td>22310</td><td>IVORY KNITTED MUG COSY </td><td>6</td><td>2010-12-01 08:34:00</td><td>1.65</td><td>13047.0</td><td>United Kingdom</td><td>9</td></tr>\n",
       "<tr><td>536367</td><td>84969</td><td>BOX OF 6 ASSORTED COLOUR TEASPOONS</td><td>6</td><td>2010-12-01 08:34:00</td><td>4.25</td><td>13047.0</td><td>United Kingdom</td><td>25</td></tr>\n",
       "<tr><td>536367</td><td>22623</td><td>BOX OF VINTAGE JIGSAW BLOCKS </td><td>3</td><td>2010-12-01 08:34:00</td><td>4.95</td><td>13047.0</td><td>United Kingdom</td><td>14</td></tr>\n",
       "<tr><td>536367</td><td>22622</td><td>BOX OF VINTAGE ALPHABET BLOCKS</td><td>2</td><td>2010-12-01 08:34:00</td><td>9.95</td><td>13047.0</td><td>United Kingdom</td><td>19</td></tr>\n",
       "<tr><td>536367</td><td>21754</td><td>HOME BUILDING BLOCK WORD</td><td>3</td><td>2010-12-01 08:34:00</td><td>5.95</td><td>13047.0</td><td>United Kingdom</td><td>17</td></tr>\n",
       "<tr><td>536367</td><td>21755</td><td>LOVE BUILDING BLOCK WORD</td><td>3</td><td>2010-12-01 08:34:00</td><td>5.95</td><td>13047.0</td><td>United Kingdom</td><td>17</td></tr>\n",
       "<tr><td>536367</td><td>21777</td><td>RECIPE BOX WITH METAL HEART</td><td>4</td><td>2010-12-01 08:34:00</td><td>7.95</td><td>13047.0</td><td>United Kingdom</td><td>31</td></tr>\n",
       "<tr><td>536367</td><td>48187</td><td>DOORMAT NEW ENGLAND</td><td>4</td><td>2010-12-01 08:34:00</td><td>7.95</td><td>13047.0</td><td>United Kingdom</td><td>31</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
       "|InvoiceNo|StockCode|                       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|TotalPrice|\n",
       "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
       "|   536367|    84879|     ASSORTED COLOUR BIRD ORNAMENT|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|        54|\n",
       "|   536367|    22745|        POPPY'S PLAYHOUSE BEDROOM |       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|        12|\n",
       "|   536367|    22748|         POPPY'S PLAYHOUSE KITCHEN|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|        12|\n",
       "|   536367|    22749| FELTCRAFT PRINCESS CHARLOTTE DOLL|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|        30|\n",
       "|   536367|    22310|           IVORY KNITTED MUG COSY |       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|         9|\n",
       "|   536367|    84969|BOX OF 6 ASSORTED COLOUR TEASPOONS|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|        25|\n",
       "|   536367|    22623|     BOX OF VINTAGE JIGSAW BLOCKS |       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|        14|\n",
       "|   536367|    22622|    BOX OF VINTAGE ALPHABET BLOCKS|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|        19|\n",
       "|   536367|    21754|          HOME BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|        17|\n",
       "|   536367|    21755|          LOVE BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|        17|\n",
       "|   536367|    21777|       RECIPE BOX WITH METAL HEART|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|        31|\n",
       "|   536367|    48187|               DOORMAT NEW ENGLAND|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|        31|\n",
       "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+----------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df2 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df2.printSchema()\n",
    "df2.show(10)\n",
    "answer = df2.where(\"InvoiceNo = '536367'\").withColumn(\"TotalPrice\", expr(\"floor(Quantity * UnitPrice)\"))\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문자열 데이터 타입 다루기\n",
    "### 4.1 첫 문자열만 대문자로 변경\n",
    "* 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경, initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 대문자(upper), 소문자(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "df.selectExpr(\"Description\", \"lower(Description)\", \"upper(Description)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 문자열 주변의 공백을 제거, lpad/ltrim/rpad/rtrim/trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO   \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>3. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 송장번호(InvoiceNo) 가 '536365' 인 거래 내역의\n",
    "#### 4. 제품코드(StockCode) 를 출력하되 총 8자리 문자로 출력하되 빈 앞자리는 0으로 채워주세요\n",
    "#### 5. 0이 패딩된 제품코드(StockCode) 컬럼의 컬럼명은 StockCode 로 유지되어야 합니다\n",
    "#### 5. 최종 출력되는 컬럼은 \"InvoiceNo\", \"StockCode\", \"Description\" 만 출력하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show(10)\n",
    "answer = df3.where(\"InvoiceNo = '536365'\").select(\"InvoiceNo\", lpad(\"StockCode\", 8, \"0\").alias(\"StockCode\"), \"Description\")\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th></tr>\n",
       "<tr><td>536365</td><td>0085123A</td><td>WHITE HANGING HEART T-LIGHT HOLDER</td></tr>\n",
       "<tr><td>536365</td><td>00071053</td><td>WHITE METAL LANTERN</td></tr>\n",
       "<tr><td>536365</td><td>0084406B</td><td>CREAM CUPID HEARTS COAT HANGER</td></tr>\n",
       "<tr><td>536365</td><td>0084029G</td><td>KNITTED UNION FLAG HOT WATER BOTTLE</td></tr>\n",
       "<tr><td>536365</td><td>0084029E</td><td>RED WOOLLY HOTTIE WHITE HEART.</td></tr>\n",
       "<tr><td>536365</td><td>00022752</td><td>SET 7 BABUSHKA NESTING BOXES</td></tr>\n",
       "<tr><td>536365</td><td>00021730</td><td>GLASS STAR FROSTED T-LIGHT HOLDER</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+-----------------------------------+\n",
       "|InvoiceNo|StockCode|                        Description|\n",
       "+---------+---------+-----------------------------------+\n",
       "|   536365| 0085123A| WHITE HANGING HEART T-LIGHT HOLDER|\n",
       "|   536365| 00071053|                WHITE METAL LANTERN|\n",
       "|   536365| 0084406B|     CREAM CUPID HEARTS COAT HANGER|\n",
       "|   536365| 0084029G|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
       "|   536365| 0084029E|     RED WOOLLY HOTTIE WHITE HEART.|\n",
       "|   536365| 00022752|       SET 7 BABUSHKA NESTING BOXES|\n",
       "|   536365| 00021730|  GLASS STAR FROSTED T-LIGHT HOLDER|\n",
       "+---------+---------+-----------------------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df3 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show(10)\n",
    "answer = df3.where(\"InvoiceNo = '536365'\").select(\"InvoiceNo\", lpad(\"StockCode\", 8, \"0\").alias(\"StockCode\"), \"Description\")\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 정규 표현식\n",
    "\n",
    "* 존재 여부를 확인하거나 일치하는 모든 문자열을 치환\n",
    "* 정규 표현식을 위해 regexp_extract 함수와 regexp_replace 함수를 제공\n",
    "\n",
    "### 5.1 단어 치환, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GRENN|BLUE\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"), col(\"Description\")).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------+\n",
      "|Description                        |Translated                    |\n",
      "+-----------------------------------+------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |WHI2 HANGING H2AR -1IGH HO1D2R|\n",
      "|WHITE METAL LANTERN                |WHI2 M2A1 1AN2RN              |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CR2AM CUPID H2ARS COA HANG2R  |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNI2D UNION F1AG HO WA2R BO12 |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |R2D WOO11Y HOI2 WHI2 H2AR.    |\n",
      "+-----------------------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자 치환, translate \"\"\"\n",
    "from pyspark.sql.functions import translate\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    translate(col(\"Description\"), \"LEET\", \"12\").alias(\"Translated\") # 정확히 매칭되지 않아도 부분만 적용됩니다 L:1, E:2\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+\n",
      "|Description                       |Extracted|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|WHITE    |\n",
      "|WHITE METAL LANTERN               |WHITE    |\n",
      "+----------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 추출, regexp_extract \"\"\"\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "extract_str = \"(BLACK|WHITE|RED|GRENN|BLUE)\"\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    regexp_extract(col(\"Description\"), extract_str, 1).alias(\"Extracted\")\n",
    ").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|WOOD 2 DRAWER CABINET WHITE FINISH|\n",
      "|WOOD S/3 CABINET ANT WHITE FINISH |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 존재유무, contain \"\"\" # 파이썬과 SQL은 instr 함수를 사용\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containBlack = instr(col(\"Description\"), \"BLACK\") > 1\n",
    "containWhite = instr(col(\"Description\"), \"WHITE\") > 1\n",
    "df.withColumn(\"hasSimpleColor\", containBlack | containWhite) \\\n",
    "    .where(\"hasSimpleColor\") \\\n",
    "    .select(\"Description\") \\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 날짜와 타임스탬프 데이터 타입 다루기\n",
    "> 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능 <br>\n",
    "> TimestampType 클래스는 초 단위 정밀도만 지원 - 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요 <br>\n",
    "\n",
    "* 스파크는 2가지 시간 정보만 다룸\n",
    "  - 날짜 정보만 가지는 date\n",
    "  - 날짜와 시간 정보를 모두 가지는 timestamp\n",
    "* 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능\n",
    "  - 자바 TimeZone 포맷을 따라야 함\n",
    "* TimestampType 클래스는 초 단위 정밀도만 지원\n",
    "  - 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요\n",
    "\n",
    "### 6.1 오늘 날짜 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|1  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|2  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|3  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|4  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|5  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|6  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|7  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|8  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "|9  |2022-04-14|2022-04-14 11:24:20.731|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = (\n",
    "    spark.range(10)\n",
    "    .withColumn(\"today\", current_date())\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    ")\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dataTable\")\n",
    "dateDF.printSchema()\n",
    "\n",
    "dateDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 날짜를 더하거나 빼기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2022-04-09|        2022-04-19|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub, date_add\n",
    "dateDF.select(\n",
    "    date_sub(col(\"today\"), 5),\n",
    "    date_add(col(\"today\"), 5)\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 두 날짜 사이의 일/개월 수를 파악 \"\"\"\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "(\n",
    "    dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\")))\n",
    ").show(1) # 현재 날짜에서 7일 제외 후 datediff 결과 확인\n",
    "\n",
    "(\n",
    "    dateDF\n",
    "    .select(to_date(lit(\"2016-01-01\")).alias(\"start\"), to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "    .select(months_between(col(\"start\"), col(\"end\")))\n",
    ").show(1) # 개월 수 차이 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자열을 날짜로 변환 \"\"\" # 자바의 simpleDateFormat 클래스가 지원하는 포맷 사용 필요\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date\"))) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               null|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|date      |date2     |\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" SimpleDateFormat 표준을 활용하여 날짜 포멧을 지정 \"\"\"\n",
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\" # 소문자 mm 주의\n",
    "cleanDateDF = spark.range(1).select( # 1개 Row를 생성\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "cleanDateDF.printSchema()\n",
    "\n",
    "cleanDateDF.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ SimpleDateFormat : https://bvc12.tistory.com/168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 문자열을 날짜로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date\"))) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               null|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>4. [고급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 적재일자(LoadDate) 컬럼을 넣되 포맷은 'yyyy-MM-dd' 으로 추가해 주시고 현재 일자를 넣으시면 됩니다\n",
    "#### 4. 송장일자(InvoiceDate) 와 오늘 시간과의 차이를 나타내는 컬럼(InvoiceDiff)을 표현식(`LoadDate - to_date(InvoiceDate)`)넣어주세요 (힌트: withColumn(\"컬럼명\", \"표현식\"))\n",
    "#### 5. 변경된 스키마를 출력하세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df4 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df4.printSchema()\n",
    "df4.show(10)\n",
    "answer = df4.withColumn(\"LoadDate\", current_date()).withColumn(\"InvoiceDiff\", expr(\"LoadDate - to_date(InvoiceDate)\"))\n",
    "display(answer)\n",
    "answer.printSchema()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+-----------+\n",
       "|InvoiceNo|StockCode|                        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|  LoadDate|InvoiceDiff|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+-----------+\n",
       "|   536365|   85123A| WHITE HANGING HEART T-LIGHT HOLDER|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536365|    71053|                WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536365|   84406B|     CREAM CUPID HEARTS COAT HANGER|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536365|   84029G|KNITTED UNION FLAG HOT WATER BOTTLE|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536365|   84029E|     RED WOOLLY HOTTIE WHITE HEART.|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536365|    22752|       SET 7 BABUSHKA NESTING BOXES|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536365|    21730|  GLASS STAR FROSTED T-LIGHT HOLDER|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536366|    22633|             HAND WARMER UNION JACK|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536366|    22632|          HAND WARMER RED POLKA DOT|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    84879|      ASSORTED COLOUR BIRD ORNAMENT|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    22745|         POPPY'S PLAYHOUSE BEDROOM |       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    22748|          POPPY'S PLAYHOUSE KITCHEN|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    22749|  FELTCRAFT PRINCESS CHARLOTTE DOLL|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    22310|            IVORY KNITTED MUG COSY |       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    84969| BOX OF 6 ASSORTED COLOUR TEASPOONS|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    22623|      BOX OF VINTAGE JIGSAW BLOCKS |       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    22622|     BOX OF VINTAGE ALPHABET BLOCKS|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    21754|           HOME BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    21755|           LOVE BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "|   536367|    21777|        RECIPE BOX WITH METAL HEART|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|14-04-2022|       null|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df4 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "# df4.printSchema()\n",
    "# df4.show(10)\n",
    "answer = df4.withColumn(\"LoadDate\", current_date()).withColumn(\"InvoiceDiff\", expr(\"LoadDate - to_date(InvoiceDate)\"))\n",
    "# display(answer)\n",
    "# answer.printSchema()\n",
    "\n",
    "\n",
    "euro_dateFormat = \"dd-MM-yyyy\" # 소문자 mm 주의\n",
    "answer2 = df4.withColumn(\"LoadDate\", date_format(current_date(),euro_dateFormat)).withColumn(\"InvoiceDiff\", expr(\"LoadDate - to_date(InvoiceDate)\"))\n",
    "display(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용 패턴에 따라 참고\n",
    "- function 공식문서\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 널 값 다루기\n",
    "+ null 값을 사용하는 것 보다 명시적으로 사용하는 것이 항상 좋음\n",
    "+ null 값을 허용하지 않는 컬럼을 선언해도 강제성은 없음\n",
    "+ nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 돕는 역할\n",
    "+ null 값을 다루는 방법은 두 가지 \n",
    "    + 명시적으로 null을 제거\n",
    "    + 전역 또는 컬럼 단위로 null 값을 특정 값으로 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1. 컬럼 값에 따른 널 처리 함수 (ifnull, nullIf, nvl, nvl2)\n",
    "+ SQL 함수이며 DataFrame의 select 표현식으로 사용 가능\n",
    "참고. http://www.gurubee.net/lecture/1880\n",
    "\n",
    "    + ifnull(expr, 'return_value') # expr이 null 이라면, 두 번째 값을, 아니라면 첫 번째 값을 반환 \n",
    "    + nullif('value', 'value')     # 두 값이 같으면 null 반환\n",
    "    + nvl(expr, 'return_value')    # expr이 null 이라면, 두 번째 값을, 아니라면 첫 번째 값을 반환. ifnull 과 같음\n",
    "    + nvl2(expr, 'return_value', 'else_value') # expr이 null 이라면, 두 번째 값을, 아니라면 세번째 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "|ifnull(NULL, return_value)|nullif(value, value)|nvl(NULL, return_value)|nvl2(not null, return_value, else_value)|\n",
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "|              return_value|                null|           return_value|                            return_value|\n",
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not null', 'return_value', 'else_value')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2 컬럼의 널 값에 따른 로우 제거 (na.drop)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameNaFunctions.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\").show(1) # 로우 컬럼값 중 하나라도 null이면 제거\n",
    "df.na.drop(\"all\").show(1) # 로우 컬럼값 모두 null이면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배열 형태의 컬럼을 인수로 전달하여 지정한 컬럼만 제거합니다\n",
    "df.na.drop(\"all\", subset=(\"StockCode\", \"InvoiceNo\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 컬럼의 널 값에 따른 값을 채움 (na.fill)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameNaFunctions.fill.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|   not_null|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null을 포함한 DataFrame 행성 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"string_null\", StringType(), True),\n",
    "    StructField(\"string2_null\", StringType(), True),\n",
    "    StructField(\"number_null\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "myRows = []\n",
    "myRows.append(Row(\"Hello\", None, float(5))) # string 컬럼에 null 포함\n",
    "myRows.append(Row(None, \"World\", None))     # number 컬럼에 null 포함\n",
    "\n",
    "myDf = spark.createDataFrame(myRows, myManualSchema)\n",
    "myDf.show()\n",
    "\n",
    "myDf.na.fill( {\"number_null\": 5.0, \"string_null\": \"not_null\"} ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 조건에 따라 다른 값으로 대체 (na.replace)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameNaFunctions.replace.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 조건에 따라 다른 값으로 대체 \"\"\"\n",
    "myDf.na.replace([\"\"], [\"Hello\"], \"string_null\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 복합 데이터 다루기\n",
    "> 구조체, 배열, 맵 등을 스파크에서 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 구조체\n",
    "* DataFrame 내부의 DataFrame\n",
    "  - 다수의 컬럼을 괄호로 묶어 생성 가능\n",
    "  - 문법에 점(.)을 사용하거나 getField 메서드를 사용\n",
    "  - (*) 문자로 모든 값을 조회할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365} |\n",
      "|{WHITE METAL LANTERN, 536365}                |\n",
      "|{CREAM CUPID HEARTS COAT HANGER, 536365}     |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE, 536365}|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART., 536365}     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- Description: string (nullable = true)\n",
      " |    |-- InvoiceNo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show(5, False)\n",
    "complexDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\", \"complex.InvoiceNo\") # 모두 동일\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"), col(\"complex\").getField(\"InvoiceNo\"))\n",
    "complexDF.select(\"complex.*\")\n",
    "complexDF.select(col(\"complex.*\"))\n",
    "complexDF.selectExpr(\"complex.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 배열\n",
    "> 데이터에서 Description 컬럼의 모든 단어를 하나의 로우로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼을 구분자로 분리하여 배열로 변환 (split)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 컬럼을 배열로 변환 \"\"\"\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 배열값의 조회 \"\"\"\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배열의 길이 (size)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.size.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" size 함수 \"\"\"\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배열에 특정 값이 존재하는지 확인 (array_contains)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.array_contains.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼의 배열값에 포함된 모든 값을 로우로 변환 (explode)\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.explode.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- splitted: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- exploded: string (nullable = true)\n",
      "\n",
      "3108\n",
      "14414\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "exploded = df \\\n",
    "    .withColumn(\"splitted\", split(col(\"Description\"), \" \")) \\\n",
    "    .withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    "exploded.printSchema()\n",
    "\n",
    "ef = exploded.select(\"Description\", \"InvoiceNo\", \"exploded\") # 모든 단어가 하나의 로우로 전환됨\n",
    "print(df.select(\"Description\").count())\n",
    "print(ef.select(\"exploded\").count()) # 로우 수가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14414"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded.select(\"Description\", \"exploded\").count() # 큰 쪽으로 카운드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='WHITE'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HANGING'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HEART'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='T-LIGHT'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HOLDER'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='WHITE'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='METAL'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='LANTERN'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CREAM'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CUPID')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded.select(\"Description\", \"exploded\").take(10) # Description 컬럼이 Group이 되어 중복됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 맵\n",
    "+ map 함수와 컬럼의 키-값 쌍을 이용해 생성\n",
    "+ 적합한 키를 사용해 데이터를 조회할 수 있으며, 해당키가 없다면 null값을 반환\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.create_map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|complex_map                                    |\n",
      "+-----------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365} |\n",
      "|{WHITE METAL LANTERN -> 536365}                |\n",
      "|{CREAM CUPID HEARTS COAT HANGER -> 536365}     |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE -> 536365}|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART. -> 536365}     |\n",
      "|{SET 7 BABUSHKA NESTING BOXES -> 536365}       |\n",
      "|{GLASS STAR FROSTED T-LIGHT HOLDER -> 536365}  |\n",
      "|{HAND WARMER UNION JACK -> 536366}             |\n",
      "|{HAND WARMER RED POLKA DOT -> 536366}          |\n",
      "|{ASSORTED COLOUR BIRD ORNAMENT -> 536367}      |\n",
      "|{POPPY'S PLAYHOUSE BEDROOM  -> 536367}         |\n",
      "|{POPPY'S PLAYHOUSE KITCHEN -> 536367}          |\n",
      "|{FELTCRAFT PRINCESS CHARLOTTE DOLL -> 536367}  |\n",
      "|{IVORY KNITTED MUG COSY  -> 536367}            |\n",
      "|{BOX OF 6 ASSORTED COLOUR TEASPOONS -> 536367} |\n",
      "|{BOX OF VINTAGE JIGSAW BLOCKS  -> 536367}      |\n",
      "|{BOX OF VINTAGE ALPHABET BLOCKS -> 536367}     |\n",
      "|{HOME BUILDING BLOCK WORD -> 536367}           |\n",
      "|{LOVE BUILDING BLOCK WORD -> 536367}           |\n",
      "|{RECIPE BOX WITH METAL HEART -> 536367}        |\n",
      "+-----------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵 생성 \"\"\"\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex_map: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                          536365|\n",
      "|                          536373|\n",
      "|                          536375|\n",
      "|                          536396|\n",
      "|                          536406|\n",
      "|                          536544|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 데이터 조회 \"\"\"\n",
    "mapped = df \\\n",
    "    .select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "mapped.printSchema()\n",
    "mapped.selectExpr(\"complex_map['WHITE METAL LANTERN']\").where(\"complex_map['WHITE METAL LANTERN'] is not null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.explode.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "|CREAM CUPID HEART...|536365|\n",
      "|KNITTED UNION FLA...|536365|\n",
      "|RED WOOLLY HOTTIE...|536365|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 분해 \"\"\"\n",
    "exploded = df \\\n",
    "    .select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .selectExpr(\"explode(complex_map)\")\n",
    "exploded.printSchema()\n",
    "exploded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. JSON 다루기\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.get_json_object.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Json 컬럼 생성 \"\"\"\n",
    "jsonDF = spark.range(1).selectExpr(\n",
    "    \"\"\"\n",
    "    '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|  null|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 인라인 쿼리로 JSON 조회하기 \"\"\"\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"jsonString.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}'),\n",
       " Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}'),\n",
       " Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" StructType을 Json 문자열로 변경 \"\"\"\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\"))) \\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJSON)|             newJSON|\n",
      "+--------------------+--------------------+\n",
      "|{536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|{536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Json 문자열을 객체로 변환 \"\"\"\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) \\\n",
    "    .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")) \\\n",
    "    .show(2) # 키를 컬럼명으로 값을 로우로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 사용자 정의 함수 \n",
    "> User defined function(UDF)는 레포트별로 데이터를 처리하는 함수이며, SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됨\n",
    "* 내장 함수가 제공하는 코드 생성 기능의 장점을 활용할 수 없어 약간의 성능 저하 발생\n",
    "* 언어별로 성능차이가 존재, 파이썬에서도 사용할 수 있으므로 자바나 스칼라도 함수 작성을 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" UDF 사용하기 \"\"\"\n",
    "udfExDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UDF 등록 및 사용 \"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)\n",
    "\n",
    "udfExDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>5. [추가]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 고객구분자(CustomerID)와 설명(Description) 컬럼이 널값인 데이터프레임을 추출하여 출력하세요\n",
    "#### 4. 고객구분자(CustomerID)가 null 인 경우는 0.0 으로 치환하고\n",
    "#### 5. 설명(Description)가 null 인 경우는 \"NOT MENTIONED\" 값으로 저장될 수 있도록 만들어주세요\n",
    "#### 6. 최종 스키마와 데이터를 출력해 주세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df5 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ").where(expr(\"Description is null or CustomerID is null\"))\n",
    "df5.printSchema()\n",
    "df5.show(10)\n",
    "desc_custid_fill = {\"Description\":\"NOT MENTIONED\", \"CustomerID\":0.0}\n",
    "answer = df5.na.fill(desc_custid_fill)\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|                null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536544|    21773|DECORATIVE ROSE B...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21774|DECORATIVE CATS B...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21786|  POLKADOT RAIN HAT |       4|2010-12-01 14:32:00|     0.85|      null|United Kingdom|\n",
      "|   536544|    21787|RAIN PONCHO RETRO...|       2|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n",
      "|   536544|    21790|  VINTAGE SNAP CARDS|       9|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n",
      "|   536544|    21791|VINTAGE HEADS AND...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21801|CHRISTMAS TREE DE...|      10|2010-12-01 14:32:00|     0.43|      null|United Kingdom|\n",
      "|   536544|    21802|CHRISTMAS TREE HE...|       9|2010-12-01 14:32:00|     0.43|      null|United Kingdom|\n",
      "|   536544|    21803|CHRISTMAS TREE ST...|      11|2010-12-01 14:32:00|     0.43|      null|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = false)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th></tr>\n",
       "<tr><td>536414</td><td>22139</td><td>NOT MENTIONED</td><td>56</td><td>2010-12-01 11:52:00</td><td>0.0</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21773</td><td>DECORATIVE ROSE BATHROOM BOTTLE</td><td>1</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21774</td><td>DECORATIVE CATS BATHROOM BOTTLE</td><td>2</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21786</td><td>POLKADOT RAIN HAT </td><td>4</td><td>2010-12-01 14:32:00</td><td>0.85</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21787</td><td>RAIN PONCHO RETROSPOT</td><td>2</td><td>2010-12-01 14:32:00</td><td>1.66</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21790</td><td>VINTAGE SNAP CARDS</td><td>9</td><td>2010-12-01 14:32:00</td><td>1.66</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21791</td><td>VINTAGE HEADS AND TAILS CARD GAME </td><td>2</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21801</td><td>CHRISTMAS TREE DECORATION WITH BELL</td><td>10</td><td>2010-12-01 14:32:00</td><td>0.43</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21802</td><td>CHRISTMAS TREE HEART DECORATION</td><td>9</td><td>2010-12-01 14:32:00</td><td>0.43</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21803</td><td>CHRISTMAS TREE STAR DECORATION</td><td>11</td><td>2010-12-01 14:32:00</td><td>0.43</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21809</td><td>CHRISTMAS HANGING TREE WITH BELL</td><td>1</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21810</td><td>CHRISTMAS HANGING STAR WITH BELL</td><td>3</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21811</td><td>CHRISTMAS HANGING HEART WITH BELL</td><td>1</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21821</td><td>GLITTER STAR GARLAND WITH BELLS </td><td>1</td><td>2010-12-01 14:32:00</td><td>7.62</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21822</td><td>GLITTER CHRISTMAS TREE WITH BELLS</td><td>1</td><td>2010-12-01 14:32:00</td><td>4.21</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21823</td><td>PAINTED METAL HEART WITH HOLLY BELL</td><td>2</td><td>2010-12-01 14:32:00</td><td>2.98</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21844</td><td>RED RETROSPOT MUG</td><td>2</td><td>2010-12-01 14:32:00</td><td>5.91</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21851</td><td>LILAC DIAMANTE PEN IN GIFT BOX</td><td>1</td><td>2010-12-01 14:32:00</td><td>4.21</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21870</td><td>I CAN ONLY PLEASE ONE PERSON MUG</td><td>1</td><td>2010-12-01 14:32:00</td><td>3.36</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21871</td><td>SAVE THE PLANET MUG</td><td>5</td><td>2010-12-01 14:32:00</td><td>3.36</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
       "|InvoiceNo|StockCode|                        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
       "|   536414|    22139|                      NOT MENTIONED|      56|2010-12-01 11:52:00|      0.0|       0.0|United Kingdom|\n",
       "|   536544|    21773|    DECORATIVE ROSE BATHROOM BOTTLE|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21774|    DECORATIVE CATS BATHROOM BOTTLE|       2|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21786|                 POLKADOT RAIN HAT |       4|2010-12-01 14:32:00|     0.85|       0.0|United Kingdom|\n",
       "|   536544|    21787|              RAIN PONCHO RETROSPOT|       2|2010-12-01 14:32:00|     1.66|       0.0|United Kingdom|\n",
       "|   536544|    21790|                 VINTAGE SNAP CARDS|       9|2010-12-01 14:32:00|     1.66|       0.0|United Kingdom|\n",
       "|   536544|    21791| VINTAGE HEADS AND TAILS CARD GAME |       2|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21801|CHRISTMAS TREE DECORATION WITH BELL|      10|2010-12-01 14:32:00|     0.43|       0.0|United Kingdom|\n",
       "|   536544|    21802|    CHRISTMAS TREE HEART DECORATION|       9|2010-12-01 14:32:00|     0.43|       0.0|United Kingdom|\n",
       "|   536544|    21803|     CHRISTMAS TREE STAR DECORATION|      11|2010-12-01 14:32:00|     0.43|       0.0|United Kingdom|\n",
       "|   536544|    21809|   CHRISTMAS HANGING TREE WITH BELL|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21810|   CHRISTMAS HANGING STAR WITH BELL|       3|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21811|  CHRISTMAS HANGING HEART WITH BELL|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21821|   GLITTER STAR GARLAND WITH BELLS |       1|2010-12-01 14:32:00|     7.62|       0.0|United Kingdom|\n",
       "|   536544|    21822|  GLITTER CHRISTMAS TREE WITH BELLS|       1|2010-12-01 14:32:00|     4.21|       0.0|United Kingdom|\n",
       "|   536544|    21823|PAINTED METAL HEART WITH HOLLY BELL|       2|2010-12-01 14:32:00|     2.98|       0.0|United Kingdom|\n",
       "|   536544|    21844|                  RED RETROSPOT MUG|       2|2010-12-01 14:32:00|     5.91|       0.0|United Kingdom|\n",
       "|   536544|    21851|     LILAC DIAMANTE PEN IN GIFT BOX|       1|2010-12-01 14:32:00|     4.21|       0.0|United Kingdom|\n",
       "|   536544|    21870|   I CAN ONLY PLEASE ONE PERSON MUG|       1|2010-12-01 14:32:00|     3.36|       0.0|United Kingdom|\n",
       "|   536544|    21871|                SAVE THE PLANET MUG|       5|2010-12-01 14:32:00|     3.36|       0.0|United Kingdom|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df5 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ").where(expr(\"Description is null or CustomerID is null\"))\n",
    "df5.printSchema()\n",
    "df5.show(10)\n",
    "desc_custid_fill = {\"Description\":\"NOT MENTIONED\", \"CustomerID\":0.0}\n",
    "answer = df5.na.fill(desc_custid_fill)\n",
    "answer.printSchema()\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>\n",
    "#### 4. [PySpark Search](https://spark.apache.org/docs/latest/api/python/search.html)\n",
    "#### 5. [Pyspark Functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#module-pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
