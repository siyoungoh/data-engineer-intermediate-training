{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1교시 스파크 기본 명령어\n",
    "\n",
    "> 스파크의 기본 명령어와 구조에 대해 이해합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 스파크를 통한 CSV 파일 읽기](#1.-스파크를-통한-CSV-파일-읽기)\n",
    "* [2. 스파크의 2가지 프로그래밍 방식 비교](#2.-스파크의-2가지-프로그래밍-방식-비교)\n",
    "* [3. 스파크를 통한 JSON 파일 읽기](#3.-스파크를-통한-JSON-파일-읽기)\n",
    "* [4. 뷰 테이블 생성 및 조회](#4.-뷰-테이블-생성-및-조회)\n",
    "* [5. 스파크 애플리케이션의 개념 이해](#5.-스파크-애플리케이션의-개념-이해)\n",
    "* [6. 스파크 UI](#6.-스파크-UI)\n",
    "* [7. M&M 초콜렛 분류 예제](#7.-M&M-초콜렛-분류-예제)\n",
    "* [8. 파이썬 vs 스파크](#8.-파이썬-vs-스파크)\n",
    "* [9. 스파크 데이터 API 의 특징](#9.-스파크-데이터-API-의-특징)\n",
    "* [참고자료](#참고자료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 스파크를 통한 CSV 파일 읽기\n",
    "> Spark 3.0.1 버전을 기준으로 작성되었습니다. 스파크는 2.0 버전으로 업데이트 되면서 DataFrames 은 Datasets 으로 통합되었고, 기존의 RDD 에서 사용하던 연산 및 기능과 DataFrame 에서 사용하던 것 모두 사용할 수 있습니다. 스파크 데이터 모델은 RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6) 형태로 업그레이드 되었으나, 본문에서 일부 DataFrames 와 DataSets 가 거의 유사하여, 일부 혼용되어 사용되는 경우가 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://664ec5845b66:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa279c8cdc0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "# print(work_dir)\n",
    "work_dir = work_dir[0]\n",
    "# print(work_dir)\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.6\n",
      "spark.version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "# !which python\n",
    "!/opt/conda/bin/python --version\n",
    "\n",
    "print(\"spark.version: {}\".format((spark.version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 스파크 사용 관련 팁\n",
    "\n",
    "#### 여러 줄의 코드 작성\n",
    "* python 코드의 경우 괄호로 (python) 묶으면 이스케이핑(\\) 하지 않아도 됩니다\n",
    "* sql 문의 경우  \"\"\"sql\"\"\" 으로 묶으면 이스케이핑(\\)하지 않아도 됩니다\n",
    "\n",
    "#### 데이터 출력 함수\n",
    "* DataFrame.show() - 기본 제공 함수이며, show(n=limit) 통하여 최대 출력을 직접 조정할 수 있으나, 한글 출력 시에 표가 깨지는 경우가 있습니다\n",
    "* display(DataFrame) - Ipython 함수이며, limit 출력을 위해서는 limit 를 걸어야 하지만, 한글 출력에도 깨지지 않습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|emp_id|emp_name|\n",
      "+------+--------+\n",
      "|     1|엘지전자|\n",
      "|     2|엘지화학|\n",
      "+------+--------+\n",
      "\n",
      "+------+\n",
      "|emp_id|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------+\n",
       "|emp_id|emp_name|\n",
       "+------+--------+\n",
       "|     1|엘지전자|\n",
       "|     2|엘지화학|\n",
       "+------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+\n",
       "|emp_id|\n",
       "+------+\n",
       "|     1|\n",
       "|     2|\n",
       "+------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 파이썬 코드 여러 줄 작성\n",
    "json = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(f\"{work_data}/tmp/simple.json\")\n",
    "    .limit(2)\n",
    ")\n",
    "\n",
    "## 스파크 SQL 여러 줄 작성\n",
    "json.createOrReplaceTempView(\"simple\")\n",
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from simple\n",
    "\"\"\")\n",
    "\n",
    "json.printSchema()\n",
    "emp_id = json.select(\"emp_id\")\n",
    "\n",
    "## 표준 데이터 출력함수\n",
    "json.show()\n",
    "emp_id.show()\n",
    "\n",
    "## 노트북 출력함수 \n",
    "display(json)\n",
    "display(emp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컨테이너 기반 노트북\n",
    "> 컨테이너 내부에 존재하는 파일 등에 대해 직접 접근이 가능합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3\n",
      "scrapy\n",
      "selenium\n",
      "mrjob\n",
      "pyspark\n",
      "pyarrow\n",
      "nltk\n",
      "jupyter_contrib_nbextensions\n",
      "jupyter_nbextensions_configurator\n"
     ]
    }
   ],
   "source": [
    "!cat ~/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|value                            |\n",
      "+---------------------------------+\n",
      "|boto3                            |\n",
      "|scrapy                           |\n",
      "|selenium                         |\n",
      "|mrjob                            |\n",
      "|pyspark                          |\n",
      "|pyarrow                          |\n",
      "|nltk                             |\n",
      "|jupyter_contrib_nbextensions     |\n",
      "|jupyter_nbextensions_configurator|\n",
      "+---------------------------------+\n",
      "\n",
      "count of word is 9\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strings = spark.read.text(f\"{home_jovyan}/requirements.txt\")\n",
    "strings.show(9, truncate=False)\n",
    "count = strings.count()\n",
    "print(\"count of word is {}\".format(count))\n",
    "\n",
    "strings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|               boto3|\n",
      "|              scrapy|\n",
      "|            selenium|\n",
      "|               mrjob|\n",
      "|             pyspark|\n",
      "|             pyarrow|\n",
      "|                nltk|\n",
      "|jupyter_contrib_n...|\n",
      "|jupyter_nbextensi...|\n",
      "+--------------------+\n",
      "\n",
      "Column<'value'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.column import Column\n",
    "\n",
    "assert(type(strings) == DataFrame)\n",
    "assert(type(strings.value) == Column) # 현재 strings 데이터프레임의 스키마에 value 라는 하나의 컬럼만 존재합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(strings) # 데이터프레임은 Structured API 를 통해 Row 타입의 레코드를 다루는 함수를 이용하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(strings.value) # 컬럼은 컬럼과의 비교 혹은 포함된 문자열을 다루는 contains 같은 함수를 사용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 스키마 추정 (Inference)\n",
    "\n",
    "#### 아무런 옵션을 주지 않는 경우 스파크가 알아서 컬럼 이름과 데이터 타입을 (string) 지정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|       _c0|  _c1|   _c2|\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫 번째 라인에 헤더가 포함되어 있는 경우 아래와 같이 header option 을 지정하면 컬럼 명을 가져올 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: string (nullable = true)\n",
      " |-- a_uid: string (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inferSchema 옵션으로 데이터 값을 확인하고 스파크가 데이터 타입을 추정하게 할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: integer (nullable = true)\n",
      " |-- a_uid: integer (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> \"data/flight-data/csv/2010-summary.csv\" 파일의 스키마와 데이터 10건을 출력하세요\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "    \n",
    "df1.printSchema()\n",
    "df1.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 스파크의 2가지 프로그래밍 방식 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나. 구조화된 API 호출을 통해 데이터를 출력하는 방법\n",
    "> 출력 시에 bigint 값인 날짜는 아래와 같이 from_unixtime 및 to_timestamp 함수를 통해 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "| Arrival_Time|    String_Datetime|\n",
      "+-------------+-------------------+\n",
      "|1424686735090|2015-02-23 19:18:55|\n",
      "|1424686735292|2015-02-23 19:18:55|\n",
      "|1424686735500|2015-02-23 19:18:55|\n",
      "|1424686735691|2015-02-23 19:18:55|\n",
      "|1424686735890|2015-02-23 19:18:55|\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date, col, lit\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", \"true\").json(f\"{work_data}/activity-data\")\n",
    "\n",
    "# 구조화된 API 를 통한 구문\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime')\n",
    ")\n",
    "timestamp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 둘. 표현식 형식으로 그대로 사용하여 출력하는 방법\n",
    "> 컬럼(col) 혹은 함수(concat 등)를 직접 사용하는 방식을 **구조화된 API** 를 사용한다고 말하고 SQL 구문으로 표현하는 방식을 **SQL 표현식**을 사용한다고 말합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "| Arrival_Time|    String_Datetime|\n",
      "+-------------+-------------------+\n",
      "|1424686735090|2015-02-23 19:18:55|\n",
      "|1424686735292|2015-02-23 19:18:55|\n",
      "|1424686735500|2015-02-23 19:18:55|\n",
      "|1424686735691|2015-02-23 19:18:55|\n",
      "|1424686735890|2015-02-23 19:18:55|\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Expression 통한 구문\n",
    "ts = df.selectExpr(\n",
    "    \"Arrival_Time\",\n",
    "    \"to_timestamp(from_unixtime(Arrival_Time / 1000), 'yyyy-MM-dd HH:mm:ss') as String_Datetime\"\n",
    ")\n",
    "ts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>2. [기본]</font> \"data/activity-data\" 경로에 저장된 json 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. 'Creation_Time' 컬럼을 년월일 포맷으로 'String_Creation_Date' 컬럼을 출력하세요\n",
    "> 단, Creation_Time 은 Arrival_Time 과 정밀도가 달라서 1000 이 아니라 `1000000000` 을 나누어 주어야 합니다\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "    \n",
    "df2.printSchema()\n",
    "display(df2.limit(3))\n",
    "answer = df2.limit(3).selectExpr(\n",
    "    \"Creation_Time\",\n",
    "    \"to_timestamp(from_unixtime(Creation_Time / 1000000000), 'yyyy-MM-dd HH:mm:ss') as String_Creation_Date\"\n",
    ")\n",
    "answer.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 스파크를 통한 JSON 파일 읽기\n",
    "\n",
    "> 추후에 학습하게 될 예정인 filter 및 groupBy 구문이 사용되고 있는데요, 조건을 통해 데이터를 줄이고(filter), 특정 컬럼별 집계(groupBy)를 위한 연산자입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 231:==========================================>             (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|user| count|\n",
      "+----+------+\n",
      "|   a|646627|\n",
      "|   b|729705|\n",
      "|   g|733185|\n",
      "|   c|617035|\n",
      "|   h|618415|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json = spark.read.json(f\"{work_data}/activity-data\")\n",
    "users = json.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>3. [기본]</font> \"data/activity-data\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. index 가 10000 미만의 이용자('user')별 빈도수를 구하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "    \n",
    "df3.printSchema()\n",
    "answer = df3.filter(\"index < 10000\").groupBy(\"user\").count()\n",
    "answer.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 뷰 테이블 생성 및 조회\n",
    "> 이미 생성된 데이터 프레임을 통해서 현재 세션에서만 조회 가능한 임시 뷰 테이블을 만들어 SQL 질의가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 235:==========================================>             (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|user| count|\n",
      "+----+------+\n",
      "|   e|767980|\n",
      "|   i|740227|\n",
      "|   f|736240|\n",
      "|   g|733185|\n",
      "|   b|729705|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "users.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"select * from users where count is not null and count > 9000 order by count desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>4. [기본]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. `2015_summary` 라는 임시 테이블을 생성하세요\n",
    "#### 2. spark sql 구문을 이용하여 10 건의 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df4.printSchema()\n",
    "answer = df4.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select * from 2015_summary\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON 파일을 읽는 여러가지 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인 - 3가지 모두 동일한 결과를 얻을 수 있으며 편한 방식을 선택하시면 됩니다\n",
    "df = spark.read.format(\"json\").load(f\"{work_data}/flight-data/json/2015-summary.json\") # 미국 교통통계국이 제공하는 항공운항 데이터\n",
    "df.printSchema()\n",
    "\n",
    "df2 = spark.read.load(f\"{work_data}/flight-data/json/2015-summary.json\", format=\"json\")\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = spark.read.json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 스파크 애플리케이션의 개념 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 스파크에서 반드시 알아야 할 객체와 개념\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Application | 스파크 프레임워크를 통해 빌드한 프로그램. 전체 작업을 관리하는 Driver 와 Executors 상에서 수행되는 프로그램으로 구분합니다 | - |\n",
    "| SparkSession | 스파크의 모든 기능을 사용하기 위해 생성하는 객체 | - |\n",
    "| Job | 하나의 액션(save, collect 등)을 수행하기 위해 여러개의 태스크로 구성된 병렬처리 단위 | DAG 혹은 Spark Execution Plan |\n",
    "| Stage | 하나의 잡은 다수의 스테이지라는 것으로 구성되며, 하나의 스테이지는 다수의 태스크 들로 구성됩니다 | - |\n",
    "| Task | 스파크 익스큐터에 보내지는 하나의 작업 단위 | 하나의 Core 혹은 Partition 단위의 작업 |\n",
    "\n",
    "### 5.2 스파크의 변환(Transformation)과 액션(Action)\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Transformation | 기본 데이터 structure 는 immutable 변경불가합니다. DataFrame을 변경하려면 기존 데이터프레임을 원하는 방식의 새로운 데이터프레임으로 변경해야합니다. 이런 작업을 Transformation 이라고 합니다. 모든 변환 작업들은 lazily evaluated 되며 lineage 를 유지합니다 | select, filter, join, groupBy, orderBy |\n",
    "| Action | 여태까지 지연된 변환 작업을 트리거링하는 동작을 말하며, 데이터의 조회 및 저장 등의 작업을 말합니다 | show, take, count, collect, save |\n",
    "\n",
    "> Lineage : 연속된 변환 파이프라인이 액션을 만나기 전까지의 모든 이력 혹은 히스토리 정보를 모두 저장하고 있는 객체를 말하며, 스파크는 이렇게 체인을 구성한 변환 작업을 통해 **쿼리 최적화**를 수행할 수 있으며, 데이터 불변성(Immutability)를 통해서 **내결함성(Fault Tolerance)** 을 가질 수 있습니다.\n",
    "\n",
    "### 5.3 좁은 변환과 넓은 변환 - Narrow and Wide Transformation\n",
    "> 위에서 언급한 최적화(Optimization) 과정은 여러 오퍼레이션들을 다시 여러 스테이지로 쪼개고, 이러한 스테이지들이 파티션을 교환하는 셔플링이 필요한지, 클러스터간의 데이터 교환이 필요한지 등을 결정하는 문제이며, 변환 작업은 크게 하나의 파티션 내에 수행이 가능한 **좁은 의존성(narrow dependencies)** 과 셔플링이 발생하여 클러스터 전체에 데이터 교환이 필요한 **넓은 의존성(wide dependencies)** 두 가지로 구분합니다\n",
    "\n",
    "- Narrow Transformation : 입력 파티션이 하나의 출력 파티션에만 기여하는 변환입니다.DataFrame에 여러 필터를 지정하면 모두 메모리 내에서 수행됩니다.\n",
    "- Wide Transformation : Spark가 클러스터에서 파티션을 교환하는 것을 셔플 이라고 합니다. 셔플을 수행하면 Spark가 결과를 디스크에 기록합니다. (advanced - 셔플 최적화) 셔플링이 발생하여 클러스터 전체에 데이터 교환이 필요.\n",
    "\n",
    "\n",
    "![Transformation](https://miro.medium.com/max/1400/0*k2TgMKHJh3N6jXDT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 스파크 UI\n",
    "> Default 포트는 4040 이므로 http://localhost:4040 (우리 수업에서는 vm{번호}.aiffelbiz.co.kr)에 접속하여 앞에서 배웠던 Narrow, Wide Transformation DAG를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|                            value|\n",
      "+---------------------------------+\n",
      "|                            boto3|\n",
      "|                           scrapy|\n",
      "|                         selenium|\n",
      "|                            mrjob|\n",
      "|                          pyspark|\n",
      "|                          pyarrow|\n",
      "|                             nltk|\n",
      "|     jupyter_contrib_nbextensions|\n",
      "|jupyter_nbextensions_configurator|\n",
      "+---------------------------------+\n",
      "\n",
      "+---------------------------------+\n",
      "|value                            |\n",
      "+---------------------------------+\n",
      "|boto3                            |\n",
      "|mrjob                            |\n",
      "|jupyter_contrib_nbextensions     |\n",
      "|jupyter_nbextensions_configurator|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Narrow Transformation\n",
    "strings = spark.read.text(f\"{home_jovyan}/requirements.txt\")\n",
    "print(strings)\n",
    "jupyter = strings.filter(strings.value.contains(\"b\"))\n",
    "jupyter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|u_gender|count|\n",
      "+--------+-----+\n",
      "|여      |3    |\n",
      "|남      |6    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wide Transformation\n",
    "user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{work_data}/tbl_user.csv\")\n",
    "# user.show(10)\n",
    "count = user.groupBy(\"u_gender\").count()\n",
    "count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Narrow | Wide |\n",
    "|---|---|\n",
    "|![narrow](https://databricks.com/wp-content/uploads/2018/05/Narrow-Transformation.png)|![wide](https://databricks.com/wp-content/uploads/2018/05/Wide-Transformation.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. M&M 초콜렛 분류 예제\n",
    "> [Learning Spark 2nd Edition](https://github.com/psyoblade/LearningSparkV2?organization=psyoblade&organization=psyoblade) 에서 제공하는 data bricks dataset 예제 가운데 미국의 주 별 M&M 초콜렛 판매량을 집계하는 예제를 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "|WY   |Blue  |16   |\n",
      "|CA   |Yellow|53   |\n",
      "|WA   |Green |60   |\n",
      "|OR   |Green |71   |\n",
      "|TX   |Green |68   |\n",
      "|NV   |Green |59   |\n",
      "|AZ   |Brown |95   |\n",
      "|WA   |Yellow|20   |\n",
      "|AZ   |Blue  |75   |\n",
      "|OR   |Brown |72   |\n",
      "|NV   |Red   |98   |\n",
      "|WY   |Orange|45   |\n",
      "|CO   |Blue  |52   |\n",
      "|TX   |Brown |94   |\n",
      "|CO   |Red   |82   |\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{work_data}/databricks/mnm_dataset.csv\")\n",
    "mnm_df.printSchema()\n",
    "mnm_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|WA   |Green |1779 |\n",
      "|OR   |Orange|1743 |\n",
      "|TX   |Green |1737 |\n",
      "|TX   |Red   |1725 |\n",
      "|CA   |Green |1723 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Green |1713 |\n",
      "|NV   |Orange|1712 |\n",
      "|TX   |Yellow|1703 |\n",
      "|NV   |Green |1698 |\n",
      "|AZ   |Brown |1698 |\n",
      "|CO   |Blue  |1695 |\n",
      "|WY   |Green |1695 |\n",
      "|NM   |Red   |1690 |\n",
      "|AZ   |Orange|1689 |\n",
      "|NM   |Yellow|1688 |\n",
      "|NM   |Brown |1687 |\n",
      "|UT   |Orange|1684 |\n",
      "|NM   |Green |1682 |\n",
      "|UT   |Red   |1680 |\n",
      "|AZ   |Green |1676 |\n",
      "|NV   |Yellow|1675 |\n",
      "|NV   |Blue  |1673 |\n",
      "|WA   |Red   |1671 |\n",
      "|WY   |Red   |1670 |\n",
      "|WA   |Brown |1669 |\n",
      "|NM   |Orange|1665 |\n",
      "|WY   |Blue  |1664 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WA   |Orange|1658 |\n",
      "|CA   |Orange|1657 |\n",
      "|NV   |Brown |1657 |\n",
      "|CO   |Brown |1656 |\n",
      "|CA   |Red   |1656 |\n",
      "|UT   |Blue  |1655 |\n",
      "|AZ   |Yellow|1654 |\n",
      "|TX   |Orange|1652 |\n",
      "|AZ   |Red   |1648 |\n",
      "|OR   |Blue  |1646 |\n",
      "|OR   |Red   |1645 |\n",
      "|UT   |Yellow|1645 |\n",
      "|CO   |Orange|1642 |\n",
      "|TX   |Brown |1641 |\n",
      "|NM   |Blue  |1638 |\n",
      "|AZ   |Blue  |1636 |\n",
      "|OR   |Green |1634 |\n",
      "|UT   |Brown |1631 |\n",
      "|WY   |Yellow|1626 |\n",
      "|WA   |Blue  |1625 |\n",
      "|CO   |Red   |1624 |\n",
      "|OR   |Brown |1621 |\n",
      "|TX   |Blue  |1614 |\n",
      "|OR   |Yellow|1614 |\n",
      "|NV   |Red   |1610 |\n",
      "|CA   |Blue  |1603 |\n",
      "|WY   |Orange|1595 |\n",
      "|UT   |Green |1591 |\n",
      "|WY   |Brown |1532 |\n",
      "+-----+------+-----+\n",
      "\n",
      "Total Rows = 60\n",
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|CA   |Green |1723 |\n",
      "|CA   |Brown |1718 |\n",
      "|CA   |Orange|1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CA   |Blue  |1603 |\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# We use the DataFrame high-level APIs. Note\n",
    "# that we don't use RDDs at all. Because some of Spark's\n",
    "# functions return the same object, we can chain function calls.\n",
    "# 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n",
    "# 2. Since we want to group each state and its M&M color count,\n",
    "# we use groupBy()\n",
    "# 3. Aggregate counts of all colors and groupBy() State and Color\n",
    "# 4 orderBy() in descending order\n",
    "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\") \\\n",
    ".groupBy(\"State\", \"Color\") \\\n",
    ".agg(count(\"Count\").alias(\"Total\")) \\\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "# Show the resulting aggregations for all the states and colors;\n",
    "# a total count of each color per state.\n",
    "# Note show() is an action, which will trigger the above\n",
    "# query to be executed.\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))\n",
    "\n",
    "# While the above code aggregated and counted for all\n",
    "# the states, what if we just want to see the data for\n",
    "# a single state, e.g., CA?\n",
    "# 1. Select from all rows in the DataFrame\n",
    "# 2. Filter only CA state\n",
    "# 3. groupBy() State and Color as we did above\n",
    "# 4. Aggregate the counts for each color\n",
    "# 5. orderBy() in descending order\n",
    "# Find the aggregate count for California by filtering\n",
    "ca_count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\") \\\n",
    ".where(mnm_df.State == \"CA\") \\\n",
    ".groupBy(\"State\", \"Color\") \\\n",
    ".agg(count(\"Count\").alias(\"Total\")) \\\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "# Show the resulting aggregation for California.\n",
    "# As above, show() is an action that will trigger the execution of the\n",
    "# entire computation.\n",
    "ca_count_mnm_df.show(n=10, truncate=False)\n",
    "# Stop the SparkSession\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>5. [기본]</font> \"data/tbl_user.csv\" 경로의 CSV 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. `user` 라는 임시 테이블을 생성하세요\n",
    "#### 3. spark sql 구문을 이용하여 10 건의 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_user.csv\")\n",
    ")\n",
    "    \n",
    "df5.printSchema()\n",
    "answer = df5.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>6. [기본]</font> \"data/tbl_purchase.csv\" 경로의 CSV 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. `purchase` 라는 임시 테이블을 생성하세요\n",
    "#### 3. selectExpr 구문 혹은 spark sql 구문을 이용하여 `p_time` 필드를 날짜 함수를 이용하여 식별 가능하도록 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습6] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df6 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_purchase.csv\")\n",
    ")\n",
    "    \n",
    "df6.printSchema()\n",
    "answer = df6.createOrReplaceTempView(\"purchase\")\n",
    "spark.sql(\"select from_unixtime(p_time) as p_time from purchase\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 파이썬 vs 스파크\n",
    "\n",
    "### 8.1 파이썬을 이용하여 1에서 100까지 더하는 함수를 계산합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0\n",
    "for number in range(1, 101, 1): result += number\n",
    "print(result)\n",
    "\n",
    "# 파이썬 3.0 에서는 reduce 함수를 사용할 수 있습니다\n",
    "from functools import reduce \n",
    "reduce(lambda x, y: x + y, range(101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 스파크 Structured API 를 통해서 1 ~ 100 까지 더하는 함수를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n",
      "5050\n"
     ]
    }
   ],
   "source": [
    "from operator import add  # 파이썬의 operator 의 add 함수를 그대로 사용합니다.\n",
    "sc = spark.sparkContext\n",
    "parallels = sc.parallelize((range(1, 101, 1))).reduce(add)  # 1 ~ 101 이전까지 1씩 증가하는 숫자를 분산객체인 RDD를 반드시 생성해야 여러 노드의 메모리에 객체가 생성됩니다.\n",
    "print(parallels)\n",
    "\n",
    "x = sc.parallelize((range(1, 101, 1))).reduce(lambda x,y: x+y)  # 파이썬 람다함수를 이용해서 익명함수를 직접 생성해서 전달해도 결과는 동일합니다\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 스파크 데이터 API 의 특징\n",
    "\n",
    "### 9.1 Untyped Dataset 연산자 (aka Dataframe operations)\n",
    "\n",
    "> Java/Scala 와 같은 strong type 언어와는 다르게 type 에 대한 강한 제약 없이 기본적인 데이터 연산자들을 사용할 수 있다 정도로 이해하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activity-data : 다양한 장치 (특히 가속도계 및 자이로 스코프)의 스마트 폰 및 스마트 워치 센서 판독 값으로 구성된 다양한 사람의 활동 데이터 집합입니다.\n",
    "df = spark.read.option(\"inferSchema\", \"true\").json(f\"{work_data}/activity-data/part-00079-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|            x|            y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+------------+\n",
      "|1424686735175|1424688581230073365|nexus4_2|   43|nexus4|   g|stand|-0.0025177002| -0.054229736| 0.025863647|\n",
      "|1424686735377|1424686733377625498|nexus4_1|   75|nexus4|   g|stand|-0.0039367676|   0.02507019| -0.01133728|\n",
      "|1424686735577|1424688581632874879|nexus4_2|  123|nexus4|   g|stand| 0.0017547607|-0.0093688965|0.0012969971|\n",
      "|1424686735776|1424686733780457529|nexus4_1|  155|nexus4|   g|stand| 0.0014038086|  0.014389038|-0.013473511|\n",
      "|1424686735979|1424686733981873545|nexus4_1|  195|nexus4|   g|stand|-0.0018005371|  0.004776001| 0.023910522|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 날짜와 시간은 아래와 같이 from_unixtime 및 to_timestamp 함수를 통해 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+\n",
      "| Arrival_Time|    String_Datetime|String_Date|\n",
      "+-------------+-------------------+-----------+\n",
      "|1424686735175|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735377|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735577|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735776|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735979|2015-02-23 19:18:55| 2015-02-23|\n",
      "+-------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime'),\n",
    "    to_date(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Date')\n",
    ")\n",
    "timestamp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 구조화된 API (Structued API)\n",
    "\n",
    "#### Selecting Dataframe using structured APIs\n",
    "\n",
    "> 스파크 API 이용 시에 컬럼명은 대소문자를 구분하지 않는 것이 기본설정입니다. (***spark.sql.caseSensitive is set to false***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Device|count|\n",
      "+--------+-----+\n",
      "|nexus4_2|39351|\n",
      "|nexus4_1|38637|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# 아래의 select 구문에서는 col(\"컬럼명\") 혹은 \"컬럼명\" 둘다 혼용이 가능합니다.\n",
    "df.filter(col(\"Index\") > 100).select(col(\"Arrival_time\"), col(\"Creation_Time\"), col(\"Device\")).groupBy(\"Device\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------+\n",
      "|concat(Arrival_time, Creation_Time)|Device  |\n",
      "+-----------------------------------+--------+\n",
      "|14246867355771424688581632874879   |nexus4_2|\n",
      "|14246867357761424686733780457529   |nexus4_1|\n",
      "|14246867359791424686733981873545   |nexus4_1|\n",
      "|14246867361811424686734183442148   |nexus4_1|\n",
      "|14246867363831424686734387513193   |nexus4_1|\n",
      "+-----------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Index\") > 100).select(concat(\"Arrival_time\", \"Creation_Time\"), \"Device\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+\n",
      "|Concated_Time            |Device  |\n",
      "+-------------------------+--------+\n",
      "|Arrival_timeCreation_Time|nexus4_2|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "+-------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 아래와 같이 structured api 를 통해서 복잡한 구문을 selectExpr 을 통해 좀 더 편하게 조회할 수 있습니다.\n",
    "df.filter(col(\"Index\") > 100).selectExpr(\"concat('Arrival_time', 'Creation_Time') as Concated_Time\", \"Device\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"index\") > 100).select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대부분의 구문에서 표현식을 통해 처리할 수 있도록 내부적으로 2가지 방식에 대해 모두 구현되어 있습니다. \n",
    "df.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select 뿐만 아니라 filter 의 경우도 Expression 을 사용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"index\") > 100).select(\"index\", \"user\").groupBy(\"user\").count().show()\n",
    "# 대부분의 구문에서 표현식을 통해 처리할 수 있도록 내부적으로 2가지 방식에 대해 모두 구현되어 있습니다. \n",
    "df.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 뷰 테이블 생성 및 활용\n",
    "\n",
    "#### JSON 파일을 이용하여 데이터프레임 생성하기\n",
    "> 임의의 JSON 파일로 부터 데이터프레임을 생성하고 집계를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json = spark.read.json(f\"{work_data}/activity-data/part-00079-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
    "users = json.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임시 뷰 테이블 생성하기\n",
    "\n",
    "> 이미 생성된 데이터 프레임을 통해서 현재 세션에서만 조회 가능한 임시 뷰 테이블을 만들어 SQL 질의가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   f| 9203|\n",
      "|   g| 9165|\n",
      "|   b| 9121|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"select * from users where count is not null and count > 9000 order by count desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 글로벌 뷰 테이블 생성하기\n",
    "\n",
    "> 현재 생성된 세션 외에서도 글로벌한 뷰 테이블 생성도 가능하며, global_temp 데이터베이스에 생성되어 $SELECT * FROM\\ global\\_temp.people$ 과 같은 형식으로 조회가 가능합니다. 다만, 하이브 테이블의 개념과 달리 영구적인 테이블 형태가 아니기 때문에 현재 수행하는 작업에 대해서만 사용하기를 권장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView(\"global_users\")\n",
    "users.createGlobalTempView(\"global_users\")\n",
    "spark.sql(\"select * from global_temp.global_users\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   g| 9165|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   h| 7730|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newSession = \"\"\"\n",
    "public SparkSession newSession()\n",
    "Start a new session with isolated SQL configurations, temporary tables, registered functions are isolated, \n",
    "but sharing the underlying SparkContext and cached data.\n",
    "\"\"\"\n",
    "spark.newSession().sql(\"select * from global_temp.global_users\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
