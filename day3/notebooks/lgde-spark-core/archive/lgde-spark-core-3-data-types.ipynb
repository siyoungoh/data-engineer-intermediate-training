{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3교시 데이터 타입\n",
    "\n",
    "> 스파크에서 사용되는 데이터 타입에 대해 실습합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 리터럴 타입](#1.-리터럴-타입)\n",
    "* [2. 불리언 형 데이터 타입 다루기](#2.-불리언-형-데이터-타입-다루기)\n",
    "* [3. 수치형 데이터 타입 다루기](#3.-수치형-데이터-타입-다루기)\n",
    "* [4. 문자열 데이터 타입 다루기](#4.-문자열-데이터-타입-다루기)\n",
    "* [5. 정규 표현식](#5.-정규-표현식)\n",
    "* [6. 날짜와 타임스팸프 데이터 타입 다루기](#6.-날짜와-타임스팸프-데이터-타입-다루기)\n",
    "* [7. 널 값 다루기](#7.-널-값-다루기)\n",
    "* [참고자료](#참고자료)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c1aa5f4167a8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f33685c8dc0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" DataFrame 생성 \"\"\"\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"retail\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 리터럴 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>5</th><th>five</th><th>5.0</th></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "<tr><td>5</td><td>five</td><td>5.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+----+---+\n",
       "|  5|five|5.0|\n",
       "+---+----+---+\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "|  5|five|5.0|\n",
       "+---+----+---+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 불리언 형 데이터 타입 다루기\n",
    "### 2.1 AND 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "x1 = df.where(col(\"InvoiceNO\") != 536365).select(\"InvoiceNO\", \"Description\")\n",
    "x2 = df.where(\"InvoiceNO <> 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "x3 = df.where(\"InvoiceNO = 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "\n",
    "x1.show(2)\n",
    "x2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 OR 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536370|     POST|       POSTAGE|       3|2010-12-01 08:45:00|     18.0|   12583.0|        France|\n",
      "|   536403|     POST|       POSTAGE|       1|2010-12-01 11:27:00|     15.0|   12791.0|   Netherlands|\n",
      "|   536527|     POST|       POSTAGE|       1|2010-12-01 13:04:00|     18.0|   12662.0|       Germany|\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536370|     POST|       POSTAGE|       3|2010-12-01 08:45:00|     18.0|   12583.0|        France|\n",
      "|   536403|     POST|       POSTAGE|       1|2010-12-01 11:27:00|     15.0|   12791.0|   Netherlands|\n",
      "|   536527|     POST|       POSTAGE|       1|2010-12-01 13:04:00|     18.0|   12662.0|       Germany|\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "# df.where( (col(\"UnitPrice\") > 600) | (instr(col(\"Description\"), \"POSTAGE\") >= 1) ).show()\n",
    "df.where(\"UnitPrice > 600 OR instr(Description, 'POSTAGE') >= 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ISIN - 제공된 목록에 포함되었는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|     POST|\n",
      "|       C2|\n",
      "|      DOT|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|     POST|\n",
      "|       C2|\n",
      "|      DOT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL 을 이용한 is in 구문 사용\n",
    "from pyspark.sql.functions import desc\n",
    "# df.select(\"StockCode\").where(col(\"StockCode\").isin([\"DOT\", \"POST\", \"C2\"])).distinct().show()\n",
    "df.select('StockCode').where(\"StockCode in ('DOT', 'POST', 'C2')\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 INSTR - 특정 문자열이 포함되었는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|added|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|    8|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|    8|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\"\"\" instr 함수 \"\"\"\n",
    "df.withColumn(\"added\", instr(df.Description, \"POSTAGE\")).where(\"added > 1\").show() # 8번째 글자에 'POSTAGE'가 시작됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 송장번호(InvoiceNo) 가 '536365' 이면서\n",
    "#### 4. 상품코드(StockCode) 가 ('85123A', '84406B', '84029G', '84029E') 중에 하나이면서\n",
    "#### 5. 제품단가(UnitPrice) 가 2.6 이하 혹은 3.0 이상인 경우를 출력하세요\n",
    "\n",
    "<details><summary>[실습1] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df1.printSchema()\n",
    "df1.show(10)\n",
    "answer = df1.where(\"InvoiceNo = '536365'\").where(\"StockCode in ('85123A', '84406B', '84029G', '84029E')\").where(\"UnitPrice < 2.6 or UnitPrice > 3.0\")\n",
    "answer.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>2. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 상품코드(StockCode) 가 (\"POST\", \"M\", \"DOT\", \"D\", \"C2\") 혹은 제품단가(UnitPrice) 가 30 이상인 데이터에 대하여\n",
    "#### 2. 상품코드(StockCode) 기준의 빈도(count)는 얼마인지 출력하세요\n",
    "* Structured API를 활용하여 작성하세요\n",
    "* StockCode 내림 차순으로 정렬하세요\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df2 = spark.read.csv(f\"{work_data}/retail-data/by-day/2010-12-01.csv\", inferSchema=True, header=True)\n",
    "df2.where( (col(\"StockCode\").isin([\"POST\", \"M\", \"DOT\", \"D\", \"C2\"])) | (col(\"UnitPrice\") >= 30)).groupBy(\"StockCode\").count().orderBy(desc(\"StockCode\"))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>StockCode</th><th>count</th></tr>\n",
       "<tr><td>POST</td><td>3</td></tr>\n",
       "<tr><td>M</td><td>2</td></tr>\n",
       "<tr><td>DOT</td><td>2</td></tr>\n",
       "<tr><td>D</td><td>1</td></tr>\n",
       "<tr><td>C2</td><td>1</td></tr>\n",
       "<tr><td>22947</td><td>1</td></tr>\n",
       "<tr><td>22946</td><td>1</td></tr>\n",
       "<tr><td>22847</td><td>1</td></tr>\n",
       "<tr><td>22827</td><td>1</td></tr>\n",
       "<tr><td>22803</td><td>2</td></tr>\n",
       "<tr><td>22769</td><td>1</td></tr>\n",
       "<tr><td>22503</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-----+\n",
       "|StockCode|count|\n",
       "+---------+-----+\n",
       "|     POST|    3|\n",
       "|        M|    2|\n",
       "|      DOT|    2|\n",
       "|        D|    1|\n",
       "|       C2|    1|\n",
       "|    22947|    1|\n",
       "|    22946|    1|\n",
       "|    22847|    1|\n",
       "|    22827|    1|\n",
       "|    22803|    2|\n",
       "|    22769|    1|\n",
       "|    22503|    1|\n",
       "+---------+-----+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 수치형 데이터 타입 다루기\n",
    "### 3.1 각종 함수를 표현식으로 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "df.selectExpr(\"CustomerID\", \"pow(Quantity * UnitPrice, 2) + 5 as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 지수만큼 제곱하는 pow 함수를 API를 사용해도 결과는 동일합니다 \"\"\"\n",
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "# 아래의 연산이 필요한 경우에는 반드시 column 으로 지정되어야 연산자 계산이 됩니다. (문자열 * 연산자는 없습니다)\n",
    "fabricateQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerID\"), fabricateQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 반올림(round), 올림(ceil), 버림(floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+\n",
      "|round(2.5, 0)|CEIL(2.4)|FLOOR(2.6)|\n",
      "+-------------+---------+----------+\n",
      "|            3|        3|         2|\n",
      "+-------------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.selectExpr(\"round(2.5, 0)\", \"ceil(2.4)\", \"floor(2.6)\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 요약 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|        InvoiceNo|\n",
      "+-------+-----------------+\n",
      "|  count|             3108|\n",
      "|   mean| 536516.684944841|\n",
      "| stddev|72.89447869788873|\n",
      "|    min|           536365|\n",
      "|    max|          C536548|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()\n",
    "df.describe(\"InvoiceNo\").show() # 컬럼을 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>3. [기본]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 송장번호(InvoiceNo) 가 '536367' 인 거래 내역의\n",
    "#### 4. 총 금액 (TotalPrice) = 수량(Quantity) * 단가(UnitPrice) 를 계산하여 TotalPrice 컬럼을 추가하세요\n",
    "#### 5. 단, 총 금액 (TotalPrice) 계산시에 소수점 이하는 버림으로 처리하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ")\n",
    "df3.printSchema()\n",
    "df3.show(10)\n",
    "answer = df3.where(\"InvoiceNo = '536367'\").withColumn(\"TotalPrice\", expr(\"floor(Quantity * UnitPrice)\"))\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th><th>TotalPrice</th></tr>\n",
       "<tr><td>536367</td><td>84879</td><td>ASSORTED COLOUR BIRD ORNAMENT</td><td>32</td><td>2010-12-01 08:34:00</td><td>1.69</td><td>13047.0</td><td>United Kingdom</td><td>54</td></tr>\n",
       "<tr><td>536367</td><td>22745</td><td>POPPY&#x27;S PLAYHOUSE BEDROOM </td><td>6</td><td>2010-12-01 08:34:00</td><td>2.1</td><td>13047.0</td><td>United Kingdom</td><td>12</td></tr>\n",
       "<tr><td>536367</td><td>22748</td><td>POPPY&#x27;S PLAYHOUSE KITCHEN</td><td>6</td><td>2010-12-01 08:34:00</td><td>2.1</td><td>13047.0</td><td>United Kingdom</td><td>12</td></tr>\n",
       "<tr><td>536367</td><td>22749</td><td>FELTCRAFT PRINCESS CHARLOTTE DOLL</td><td>8</td><td>2010-12-01 08:34:00</td><td>3.75</td><td>13047.0</td><td>United Kingdom</td><td>30</td></tr>\n",
       "<tr><td>536367</td><td>22310</td><td>IVORY KNITTED MUG COSY </td><td>6</td><td>2010-12-01 08:34:00</td><td>1.65</td><td>13047.0</td><td>United Kingdom</td><td>9</td></tr>\n",
       "<tr><td>536367</td><td>84969</td><td>BOX OF 6 ASSORTED COLOUR TEASPOONS</td><td>6</td><td>2010-12-01 08:34:00</td><td>4.25</td><td>13047.0</td><td>United Kingdom</td><td>25</td></tr>\n",
       "<tr><td>536367</td><td>22623</td><td>BOX OF VINTAGE JIGSAW BLOCKS </td><td>3</td><td>2010-12-01 08:34:00</td><td>4.95</td><td>13047.0</td><td>United Kingdom</td><td>14</td></tr>\n",
       "<tr><td>536367</td><td>22622</td><td>BOX OF VINTAGE ALPHABET BLOCKS</td><td>2</td><td>2010-12-01 08:34:00</td><td>9.95</td><td>13047.0</td><td>United Kingdom</td><td>19</td></tr>\n",
       "<tr><td>536367</td><td>21754</td><td>HOME BUILDING BLOCK WORD</td><td>3</td><td>2010-12-01 08:34:00</td><td>5.95</td><td>13047.0</td><td>United Kingdom</td><td>17</td></tr>\n",
       "<tr><td>536367</td><td>21755</td><td>LOVE BUILDING BLOCK WORD</td><td>3</td><td>2010-12-01 08:34:00</td><td>5.95</td><td>13047.0</td><td>United Kingdom</td><td>17</td></tr>\n",
       "<tr><td>536367</td><td>21777</td><td>RECIPE BOX WITH METAL HEART</td><td>4</td><td>2010-12-01 08:34:00</td><td>7.95</td><td>13047.0</td><td>United Kingdom</td><td>31</td></tr>\n",
       "<tr><td>536367</td><td>48187</td><td>DOORMAT NEW ENGLAND</td><td>4</td><td>2010-12-01 08:34:00</td><td>7.95</td><td>13047.0</td><td>United Kingdom</td><td>31</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
       "|InvoiceNo|StockCode|                       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|TotalPrice|\n",
       "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
       "|   536367|    84879|     ASSORTED COLOUR BIRD ORNAMENT|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|        54|\n",
       "|   536367|    22745|        POPPY'S PLAYHOUSE BEDROOM |       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|        12|\n",
       "|   536367|    22748|         POPPY'S PLAYHOUSE KITCHEN|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|        12|\n",
       "|   536367|    22749| FELTCRAFT PRINCESS CHARLOTTE DOLL|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|        30|\n",
       "|   536367|    22310|           IVORY KNITTED MUG COSY |       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|         9|\n",
       "|   536367|    84969|BOX OF 6 ASSORTED COLOUR TEASPOONS|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|        25|\n",
       "|   536367|    22623|     BOX OF VINTAGE JIGSAW BLOCKS |       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|        14|\n",
       "|   536367|    22622|    BOX OF VINTAGE ALPHABET BLOCKS|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|        19|\n",
       "|   536367|    21754|          HOME BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|        17|\n",
       "|   536367|    21755|          LOVE BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|        17|\n",
       "|   536367|    21777|       RECIPE BOX WITH METAL HEART|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|        31|\n",
       "|   536367|    48187|               DOORMAT NEW ENGLAND|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|        31|\n",
       "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+----------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문자열 데이터 타입 다루기\n",
    "### 4.1 첫 문자열만 대문자로 변경\n",
    "* 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경, initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 대문자(upper), 소문자(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "df.selectExpr(\"Description\", \"lower(Description)\", \"upper(Description)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 문자열 주변의 공백을 제거, lpad/ltrim/rpad/rtrim/trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO   \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>4. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 송장번호(InvoiceNo) 가 '536365' 인 거래 내역의 제품코드(StockCode) 를 총 8자리 문자로 출력해 주세요\n",
    "* 제품코드의 출력 시의 빈 앞자리는 0으로 채워주세요 (Padding)\n",
    "* 0이 패딩된 제품코드(StockCode) 컬럼의 컬럼명은 StockCode 로 유지되어야 합니다\n",
    "* 최종 출력되는 컬럼은 \"InvoiceNo\", \"StockCode\", \"Description\" 만 출력하세요\n",
    "* 가능한 Structured API 를 사용하여 작성하되 최대한 간결하게 작성해 보세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df4 = spark.read.csv(f\"{work_data}/retail-data/by-day/2010-12-01.csv\", inferSchema=True, header=True)\n",
    "df4.where(\"InvoiceNo = '536365'\").select(\"InvoiceNo\", lpad(\"StockCode\", 8, \"0\").alias(\"StockCode\"), \"Description\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th></tr>\n",
       "<tr><td>536365</td><td>0085123A</td><td>WHITE HANGING HEART T-LIGHT HOLDER</td></tr>\n",
       "<tr><td>536365</td><td>00071053</td><td>WHITE METAL LANTERN</td></tr>\n",
       "<tr><td>536365</td><td>0084406B</td><td>CREAM CUPID HEARTS COAT HANGER</td></tr>\n",
       "<tr><td>536365</td><td>0084029G</td><td>KNITTED UNION FLAG HOT WATER BOTTLE</td></tr>\n",
       "<tr><td>536365</td><td>0084029E</td><td>RED WOOLLY HOTTIE WHITE HEART.</td></tr>\n",
       "<tr><td>536365</td><td>00022752</td><td>SET 7 BABUSHKA NESTING BOXES</td></tr>\n",
       "<tr><td>536365</td><td>00021730</td><td>GLASS STAR FROSTED T-LIGHT HOLDER</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+-----------------------------------+\n",
       "|InvoiceNo|StockCode|                        Description|\n",
       "+---------+---------+-----------------------------------+\n",
       "|   536365| 0085123A| WHITE HANGING HEART T-LIGHT HOLDER|\n",
       "|   536365| 00071053|                WHITE METAL LANTERN|\n",
       "|   536365| 0084406B|     CREAM CUPID HEARTS COAT HANGER|\n",
       "|   536365| 0084029G|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
       "|   536365| 0084029E|     RED WOOLLY HOTTIE WHITE HEART.|\n",
       "|   536365| 00022752|       SET 7 BABUSHKA NESTING BOXES|\n",
       "|   536365| 00021730|  GLASS STAR FROSTED T-LIGHT HOLDER|\n",
       "+---------+---------+-----------------------------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. 정규 표현식 & 조건부 컬럼\n",
    "\n",
    "### 5.1 단어 치환, regexp_extract\n",
    "* 존재 여부를 확인하거나 일치하는 모든 문자열을 치환\n",
    "* 정규 표현식을 위해 regexp_extract 함수와 regexp_replace 함수를 제공\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GRENN|BLUE\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"), col(\"Description\")).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|          Translated|\n",
      "+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|WHI2 HANGING H2AR...|\n",
      "| WHITE METAL LANTERN|    WHI2 M2A1 1AN2RN|\n",
      "|CREAM CUPID HEART...|CR2AM CUPID H2ARS...|\n",
      "|KNITTED UNION FLA...|KNI2D UNION F1AG ...|\n",
      "|RED WOOLLY HOTTIE...|R2D WOO11Y HOI2 W...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자 치환, translate \"\"\"\n",
    "from pyspark.sql.functions import translate\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    translate(col(\"Description\"), \"LEET\", \"12\").alias(\"Translated\") # 정확히 매칭되지 않아도 부분만 적용됩니다 L:1, E:2\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|Extracted|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|    WHITE|\n",
      "| WHITE METAL LANTERN|    WHITE|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 추출, regexp_extract \"\"\"\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "extract_str = \"(BLACK|WHITE|RED|GRENN|BLUE)\"\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    regexp_extract(col(\"Description\"), extract_str, 1).alias(\"Extracted\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|WOOD 2 DRAWER CABINET WHITE FINISH|\n",
      "|WOOD S/3 CABINET ANT WHITE FINISH |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 존재유무, contain \"\"\" # 파이썬과 SQL은 instr 함수를 사용\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containBlack = instr(col(\"Description\"), \"BLACK\") > 1\n",
    "containWhite = instr(col(\"Description\"), \"WHITE\") > 1\n",
    "df.withColumn(\"hasSimpleColor\", containBlack | containWhite) \\\n",
    "    .where(\"hasSimpleColor\") \\\n",
    "    .select(\"Description\") \\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|is_black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   false|    true| false|   false|  false|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   false|    true| false|   false|  false|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   false|   false| false|   false|  false|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 필드에 색깔 문자열이 포함되어 있는지 여부를 locate 함수를 이용하여 컬럼으로 생성하는 에제 \"\"\"\n",
    "from pyspark.sql.functions import expr, locate \n",
    "\n",
    "simple_colors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):   # color_strings 단어가 시작되는 문자기준(단어기준 X) 위치\n",
    "    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n",
    "\n",
    "selected_cols = [color_locator(df.Description, c) for c in simple_colors] # locate 함수를 하나씩 list에 저장\n",
    "selected_cols.append(expr(\"*\")) # column 타입이여야 함 \n",
    "\n",
    "# * means -> Unnest List to variable arguments\n",
    "df.select(*selected_cols).show(3)\n",
    "\n",
    "df.select(*selected_cols).where(expr(\"is_white OR is_red\")) \\\n",
    "    .select(col(\"Description\")) \\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 조건부 컬럼 생성, when case else\n",
    "\n",
    "* IF ELSE 와 같이 조건에 부합하는 경우에 따라 컬럼을 반환\n",
    "* 조건을 중첩시켜서 N개의 조건을 적용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Quantity|  size|\n",
      "+--------+------+\n",
      "|     600| Large|\n",
      "|     480|Middle|\n",
      "|     432|Middle|\n",
      "|     432|Middle|\n",
      "|     384|Middle|\n",
      "|     288|Middle|\n",
      "|     252|Middle|\n",
      "|     216| Small|\n",
      "|     200| Small|\n",
      "|     200| Small|\n",
      "+--------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+\n",
      "|Quantity|  size|\n",
      "+--------+------+\n",
      "|     600| large|\n",
      "|     480|middle|\n",
      "|     432|middle|\n",
      "|     432|middle|\n",
      "|     384|middle|\n",
      "|     288|middle|\n",
      "|     252|middle|\n",
      "|     216| small|\n",
      "|     200| small|\n",
      "|     200| small|\n",
      "+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.withColumn(\n",
    "        \"Size\", \n",
    "        when(col(\"Quantity\") > 500, \"Large\")\n",
    "        .when(col(\"Quantity\") > 250, \"Middle\")\n",
    "        .otherwise(\"Small\")\n",
    "    )\n",
    "    .select(\"Quantity\", \"size\")\n",
    "    .orderBy(desc(\"Quantity\"))\n",
    ").show(10)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select Quantity,\n",
    "        case when Quantity > 500 then 'large' \n",
    "        when Quantity > 250 then 'middle' \n",
    "        else 'small' end as size \n",
    "    from retail \n",
    "    order by Quantity desc\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5-3. 사용자 정의 함수 \n",
    "\n",
    "> User defined function(UDF)는 레포트별로 데이터를 처리하는 함수이며, SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됨\n",
    "\n",
    "* 내장 함수가 제공하는 코드 생성 기능의 장점을 활용할 수 없어 약간의 성능 저하 발생\n",
    "* 언어별로 성능차이가 존재, 파이썬에서도 사용할 수 있으므로 자바나 스칼라도 함수 작성을 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" UDF 사용하기 \"\"\"\n",
    "udfExDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UDF 등록 및 사용 \"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)\n",
    "udfExDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>5. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 송장번호(InvoiceNo) 가 '536365' 인 거래 내역의 제품코드(StockCode) 를 총 8자리 문자로 출력해 주세요\n",
    "* 제품코드의 출력 시의 빈 앞자리는 0으로 채워주세요 (Padding)\n",
    "* 0이 패딩된 제품코드(StockCode) 컬럼의 컬럼명은 StockCode 로 유지되어야 합니다\n",
    "* 최종 출력되는 컬럼은 \"InvoiceNo\", \"StockCode\", \"Description\" 만 출력하세요\n",
    "* 가능한 Structured API 를 사용하여 작성하되 최대한 간결하게 작성해 보세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df5 = spark.read.csv(f\"{work_data}/retail-data/by-day/2010-12-01.csv\", inferSchema=True, header=True)\n",
    "df5.where(\"InvoiceNo = '536365'\").select(\"InvoiceNo\", lpad(\"StockCode\", 8, \"0\").alias(\"StockCode\"), \"Description\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th></tr>\n",
       "<tr><td>536365</td><td>0085123A</td><td>WHITE HANGING HEART T-LIGHT HOLDER</td></tr>\n",
       "<tr><td>536365</td><td>00071053</td><td>WHITE METAL LANTERN</td></tr>\n",
       "<tr><td>536365</td><td>0084406B</td><td>CREAM CUPID HEARTS COAT HANGER</td></tr>\n",
       "<tr><td>536365</td><td>0084029G</td><td>KNITTED UNION FLAG HOT WATER BOTTLE</td></tr>\n",
       "<tr><td>536365</td><td>0084029E</td><td>RED WOOLLY HOTTIE WHITE HEART.</td></tr>\n",
       "<tr><td>536365</td><td>00022752</td><td>SET 7 BABUSHKA NESTING BOXES</td></tr>\n",
       "<tr><td>536365</td><td>00021730</td><td>GLASS STAR FROSTED T-LIGHT HOLDER</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+-----------------------------------+\n",
       "|InvoiceNo|StockCode|                        Description|\n",
       "+---------+---------+-----------------------------------+\n",
       "|   536365| 0085123A| WHITE HANGING HEART T-LIGHT HOLDER|\n",
       "|   536365| 00071053|                WHITE METAL LANTERN|\n",
       "|   536365| 0084406B|     CREAM CUPID HEARTS COAT HANGER|\n",
       "|   536365| 0084029G|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
       "|   536365| 0084029E|     RED WOOLLY HOTTIE WHITE HEART.|\n",
       "|   536365| 00022752|       SET 7 BABUSHKA NESTING BOXES|\n",
       "|   536365| 00021730|  GLASS STAR FROSTED T-LIGHT HOLDER|\n",
       "+---------+---------+-----------------------------------+"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>6. [고급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. DESCRIPTION 항목에서 'GREEN' -> '초급', 'BLUE' -> '중급', 'RED' -> '고급' 으로 변경하여 출력하세요\n",
    "* 해당 함수를 colorGrade(column) 라는 함수를 이용해서 작성해 보세요\n",
    "* 최종 출력 시에는 Description 컬럼에 '급' 문자를 포함한 결과만 출력하세요\n",
    "\n",
    "<details><summary>[실습6] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df6 = spark.read.csv(f\"{work_data}/retail-data/by-day/2010-12-01.csv\", inferSchema=True, header=True)\n",
    "df6.printSchema()\n",
    "df6.show(3)\n",
    "\n",
    "def colorReplacer(column):\n",
    "    return (\n",
    "        when(column.contains(\"GREEN\"), regexp_replace(column, \"GREEN\", \"초급\"))\n",
    "            .when(column.contains(\"BLUE\"), regexp_replace(column, \"BLUE\", \"중급\"))\n",
    "            .when(column.contains(\"RED\"), regexp_replace(column, \"RED\", \"고급\"))\n",
    "            .otherwise(column).alias(\"Description\")\n",
    "    )\n",
    "    \n",
    "df6.select(colorReplacer(col(\"Description\"))).where(col(\"Description\").contains(\"급\")).show(truncate=False)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------------------------+\n",
      "|Description                        |\n",
      "+-----------------------------------+\n",
      "|고급 WOOLLY HOTTIE WHITE HEART.    |\n",
      "|HAND WARMER 고급 POLKA DOT         |\n",
      "|고급 COAT RACK PARIS FASHION       |\n",
      "|중급 COAT RACK PARIS FASHION       |\n",
      "|ALARM CLOCK BAKELIKE 고급          |\n",
      "|ALARM CLOCK BAKELIKE 초급          |\n",
      "|SET/2 고급 RETROSPOT TEA TOWELS    |\n",
      "|고급 TOADSTOOL LED NIGHT LIGHT     |\n",
      "|HAND WARMER 고급 POLKA DOT         |\n",
      "|EDWARDIAN PARASOL 고급             |\n",
      "|고급 WOOLLY HOTTIE WHITE HEART.    |\n",
      "|EDWARDIAN PARASOL 고급             |\n",
      "|고급 WOOLLY HOTTIE WHITE HEART.    |\n",
      "|고급 HANGING HEART T-LIGHT HOLDER  |\n",
      "|HAND WARMER 고급 POLKA DOT         |\n",
      "|고급 3 PIECE RETROSPOT CUTLERY SET |\n",
      "|중급 3 PIECE POLKADOT CUTLERY SET  |\n",
      "|SET/6 고급 SPOTTY PAPER PLATES     |\n",
      "|LUNCH BAG 고급 RETROSPOT           |\n",
      "|고급 CHARLIE+LOLA PERSONAL DOORSIGN|\n",
      "+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 날짜와 타임스팸프 데이터 타입 다루기\n",
    "> 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능 <br>\n",
    "> TimestampType 클래스는 초 단위 정밀도만 지원 - 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요 <br>\n",
    "\n",
    "* 스파크는 2가지 시간 정보만 다룸\n",
    "  - 날짜 정보만 가지는 date\n",
    "  - 날짜와 시간 정보를 모두 가지는 timestamp\n",
    "* 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능\n",
    "  - 자바 TimeZone 포맷을 따라야 함\n",
    "* TimestampType 클래스는 초 단위 정밀도만 지원\n",
    "  - 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요\n",
    "\n",
    "### 6.1 오늘 날짜 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2021-08-21|2021-08-21 17:49:05.713|\n",
      "|1  |2021-08-21|2021-08-21 17:49:05.713|\n",
      "|2  |2021-08-21|2021-08-21 17:49:05.713|\n",
      "+---+----------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = (\n",
    "    spark.range(10)\n",
    "    .withColumn(\"today\", current_date())\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    ")\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dataTable\")\n",
    "dateDF.printSchema()\n",
    "\n",
    "dateDF.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 날짜를 더하거나 빼기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2021-08-16|        2021-08-26|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub, date_add\n",
    "dateDF.select(\n",
    "    date_sub(col(\"today\"), 5),\n",
    "    date_add(col(\"today\"), 5)\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 두 날짜 사이의 일/개월 수를 파악 \"\"\"\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "(\n",
    "    dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\")))\n",
    ").show(1) # 현재 날짜에서 7일 제외 후 datediff 결과 확인\n",
    "\n",
    "(\n",
    "    dateDF\n",
    "    .select(to_date(lit(\"2016-01-01\")).alias(\"start\"), to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "    .select(months_between(col(\"start\"), col(\"end\")))\n",
    ").show(1) # 개월 수 차이 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자열을 날짜로 변환 \"\"\" # 자바의 simpleDateFormat 클래스가 지원하는 포맷 사용 필요\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date\"))) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               null|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SimpleDateFormat 표준을 활용하여 날짜 포멧을 지정 \"\"\"\n",
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\" # 소문자 mm 주의\n",
    "cleanDateDF = spark.range(1).select( # 1개 Row를 생성\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ SimpleDateFormat : https://bvc12.tistory.com/168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 문자열을 날짜로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date\"))) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               null|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>7. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 적재일자(LoadDate) 컬럼을 넣되 포맷은 'yyyy-MM-dd' 으로 추가해 주시고 현재 일자를 넣으시면 됩니다\n",
    "#### 2. 송장일자(InvoiceDate) 와 오늘 시간과의 차이를 나타내는 컬럼(InvoiceDiff)을 (LoadDate - to_date(InvoiceDate))넣어주세요\n",
    "* 변경된 스키마를 출력하여 동일한 지 확인해 주세요\n",
    "* 가능한 Structured API 를 사용하여 작성하되 최대한 간결하게 작성해 보세요\n",
    "\n",
    "<details><summary>[실습7] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df7 = spark.read.csv(f\"{work_data}/retail-data/by-day/2010-12-01.csv\", inferSchema=True, header=True)\n",
    "answer = df7.withColumn(\"LoadDate\", current_date()).withColumn(\"InvoiceDiff\", col(\"LoadDate\") - to_date(col(\"InvoiceDate\")))\n",
    "display(answer)\n",
    "answer.printSchema()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+-----------------+\n",
       "|InvoiceNo|StockCode|                        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|  LoadDate|      InvoiceDiff|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+-----------------+\n",
       "|   536365|   85123A| WHITE HANGING HEART T-LIGHT HOLDER|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536365|    71053|                WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536365|   84406B|     CREAM CUPID HEARTS COAT HANGER|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536365|   84029G|KNITTED UNION FLAG HOT WATER BOTTLE|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536365|   84029E|     RED WOOLLY HOTTIE WHITE HEART.|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536365|    22752|       SET 7 BABUSHKA NESTING BOXES|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536365|    21730|  GLASS STAR FROSTED T-LIGHT HOLDER|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536366|    22633|             HAND WARMER UNION JACK|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536366|    22632|          HAND WARMER RED POLKA DOT|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    84879|      ASSORTED COLOUR BIRD ORNAMENT|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    22745|         POPPY'S PLAYHOUSE BEDROOM |       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    22748|          POPPY'S PLAYHOUSE KITCHEN|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    22749|  FELTCRAFT PRINCESS CHARLOTTE DOLL|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    22310|            IVORY KNITTED MUG COSY |       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    84969| BOX OF 6 ASSORTED COLOUR TEASPOONS|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    22623|      BOX OF VINTAGE JIGSAW BLOCKS |       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    22622|     BOX OF VINTAGE ALPHABET BLOCKS|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    21754|           HOME BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    21755|           LOVE BUILDING BLOCK WORD|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "|   536367|    21777|        RECIPE BOX WITH METAL HEART|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|2021-09-01|10 years 9 months|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+-----------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LoadDate: date (nullable = false)\n",
      " |-- InvoiceDiff: interval (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 널 값 다루기\n",
    "+ null 값을 사용하는 것 보다 명시적으로 사용하는 것이 항상 좋음\n",
    "+ null 값을 허용하지 않는 컬럼을 선언해도 강제성은 없음\n",
    "+ nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 돕는 역할\n",
    "+ null 값을 다루는 방법은 두 가지 \n",
    "    + 명시적으로 null을 제거\n",
    "    + 전역 또느 컬럼 단위로 null 값을 특정 값으로 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1. 컬럼 값에 따른 널 처리 함수 (ifnull, nullIf, nvl, nvl2)\n",
    "+ SQL 함수이며 DataFrame의 select 표현식으로 사용 가능\n",
    "    + ifnull(null, 'return_value') # 두 번째 값을, 아니라면 첫 번째 값을 반환 \n",
    "    + nullif('value', 'value')     # 두 값이 같으면 null\n",
    "    + nvl(null, 'return_value')    # 두 번째 값을, 아니라면 첫 번째 값을 반환\n",
    "    + nvl2('not_null', 'return_value', 'else_value') # 두 번째 값을, 아니라면 세번째 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "|ifnull(NULL, return_value)|nullif(value, value)|nvl(NULL, return_value)|nvl2(not null, return_value, else_value)|\n",
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "|              return_value|                null|           return_value|                            return_value|\n",
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not null', 'return_value', 'else_value')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2 컬럼의 널 값에 따른 로우 제거 (na.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\").show(1) # 로우 컬럼값 중 하나라도 null이면 제거\n",
    "df.na.drop(\"all\").show(1) # 로우 컬럼값 모두 null이면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배열 형태의 컬럼을 인수로 전달하여 지정한 컬럼만 전체(all)가 null 인 경우만 제거합니다\n",
    "df.na.drop(\"all\", subset=(\"StockCode\", \"InvoiceNo\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 컬럼의 널 값에 따른 값을 채움 (na.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "|       null|        null|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "|      Hello|        null|        5.0|\n",
      "|   not_null|       World|        5.0|\n",
      "|   not_null|        null|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null을 포함한 DataFrame 행성 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"string_null\", StringType(), True),\n",
    "    StructField(\"string2_null\", StringType(), True),\n",
    "    StructField(\"number_null\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "myRows = []\n",
    "myRows.append(Row(\"Hello\", \"World\", float(5))) # string 컬럼에 null 포함\n",
    "myRows.append(Row(\"Hello\", None, float(5))) # string 컬럼에 null 포함\n",
    "myRows.append(Row(None, \"World\", None))     # number 컬럼에 null 포함\n",
    "myRows.append(Row(None, None, None))        # 모든 컬럼이 null\n",
    "\n",
    "myDf = spark.createDataFrame(myRows, myManualSchema)\n",
    "myDf.show()\n",
    "\n",
    "myDf.na.fill( {\"number_null\": 5.0, \"string_null\": \"not_null\"} ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "|       null|        null|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "|      Hello|        null|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "|      Hello|        null|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()\n",
    "myDf.na.drop(\"all\", subset=(\"string_null\", \"number_null\")).show()\n",
    "myDf.na.drop(\"all\").show()\n",
    "myDf.na.drop(\"any\", subset=(\"string_null\", \"number_null\")).show()\n",
    "myDf.na.drop(\"any\", subset=(\"string_null\", \"string2_null\", \"number_null\")).show()\n",
    "myDf.na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 조건에 따라 다른 값으로 대체 (na.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 조건에 따라 다른 값으로 대체 \"\"\"\n",
    "myDf.na.replace([\"\"], [\"Hello\"], \"string_null\").show() # null을 지정하는 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>8. [중급]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 고객아이디(CustomerID) 컬럼에 대해서만 널 검사를 하되 널값이 있다면 해당 로우는 제외해 주세요\n",
    "#### 2. 국가(Country) 값은 아래의 규칙으로 변경해 주세요\n",
    "* \"United Kingdom\" -> \"UK\", \"France\" -> \"FR\", \"Germany\" -> \"DE\", \"Netherlands\" -> \"NL\", \"Australia\" -> \"AT\", \"Norway\" -> \"NO\", \"EIRE\" -> \"IE\"\n",
    "\n",
    "#### 3. 국가(Country) 별 빈도수를 출력해 주세요\n",
    "* 국가의 빈도수 역순으로 정렬해 주세요\n",
    "* 가능한 Structured API 를 사용하여 작성하되 최대한 간결하게 작성해 주세요\n",
    "\n",
    "<details><summary>[실습8] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df8 = spark.read.csv(f\"{work_data}/retail-data/by-day/2010-12-01.csv\", inferSchema=True, header=True)\n",
    "keys = [\"United Kingdom\", \"France\", \"Germany\", \"Netherlands\", \"Australia\", \"Norway\", \"EIRE\"]\n",
    "values = [\"UK\", \"FR\", \"DE\", \"NL\", \"AT\", \"NO\", \"IE\"]\n",
    "answer = (\n",
    "    df8.na.drop(\"any\", subset=(\"CustomerID\"))\n",
    "    .na.replace(keys, values, \"Country\")\n",
    "    .groupBy(\"Country\")\n",
    "    .count().orderBy(desc(\"count\"))\n",
    ").show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Country|count|\n",
      "+-------+-----+\n",
      "|     UK| 1809|\n",
      "|     NO|   73|\n",
      "|     DE|   29|\n",
      "|     IE|   21|\n",
      "|     FR|   20|\n",
      "|     AT|   14|\n",
      "|     NL|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 복합 데이터 다루기\n",
    "> 구조체, 배열, 맵 등을 스파크에서 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 구조체\n",
    "* DataFrame 내부의 DataFrame\n",
    "  - 다수의 컬럼을 괄호로 묶어 생성 가능\n",
    "  - 문법에 점(.)을 사용하거나 getField 메서드를 사용\n",
    "  - (*) 문자로 모든 값을 조회할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365} |\n",
      "|{WHITE METAL LANTERN, 536365}                |\n",
      "|{CREAM CUPID HEARTS COAT HANGER, 536365}     |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE, 536365}|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART., 536365}     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- Description: string (nullable = true)\n",
      " |    |-- InvoiceNo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show(5, False)\n",
    "complexDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\", \"complex.InvoiceNo\") # 모두 동일\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"), col(\"complex\").getField(\"InvoiceNo\"))\n",
    "complexDF.select(\"complex.*\")\n",
    "complexDF.select(col(\"complex.*\"))\n",
    "complexDF.selectExpr(\"complex.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 배열\n",
    "> 데이터에서 Description 컬럼의 모든 단어를 하나의 로우로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼을 구분자로 분리하여 배열로 변환 (split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 컬럼을 배열로 변환 \"\"\"\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 배열값의 조회 \"\"\"\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배열의 길이 (size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" size 함수 \"\"\"\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배열에 특정 값이 존재하는지 확인 (array_contains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼의 배열값에 포함된 모든 값을 로우로 변환 (explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- splitted: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- exploded: string (nullable = true)\n",
      "\n",
      "3108\n",
      "14414\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "exploded = df \\\n",
    "    .withColumn(\"splitted\", split(col(\"Description\"), \" \")) \\\n",
    "    .withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    "exploded.printSchema()\n",
    "\n",
    "ef = exploded.select(\"Description\", \"InvoiceNo\", \"exploded\") # 모든 단어가 하나의 로우로 전환됨\n",
    "print(df.select(\"Description\").count())\n",
    "print(ef.select(\"exploded\").count()) # 로우 수가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14414"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded.select(\"Description\", \"exploded\").count() # 큰 쪽으로 카운드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='WHITE'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HANGING'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HEART'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='T-LIGHT'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HOLDER'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='WHITE'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='METAL'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='LANTERN'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CREAM'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CUPID')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded.select(\"Description\", \"exploded\").take(10) # Description 컬럼이 Group이 되어 중복됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 맵\n",
    "+ map 함수와 컬럼의 키0값 쌍을 이용해 생성\n",
    "+ 적합한 키를 사용해 데이터를 조회할 수 있으며, 해당키가 없다면 null값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|complex_map                                    |\n",
      "+-----------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365} |\n",
      "|{WHITE METAL LANTERN -> 536365}                |\n",
      "|{CREAM CUPID HEARTS COAT HANGER -> 536365}     |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE -> 536365}|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART. -> 536365}     |\n",
      "|{SET 7 BABUSHKA NESTING BOXES -> 536365}       |\n",
      "|{GLASS STAR FROSTED T-LIGHT HOLDER -> 536365}  |\n",
      "|{HAND WARMER UNION JACK -> 536366}             |\n",
      "|{HAND WARMER RED POLKA DOT -> 536366}          |\n",
      "|{ASSORTED COLOUR BIRD ORNAMENT -> 536367}      |\n",
      "|{POPPY'S PLAYHOUSE BEDROOM  -> 536367}         |\n",
      "|{POPPY'S PLAYHOUSE KITCHEN -> 536367}          |\n",
      "|{FELTCRAFT PRINCESS CHARLOTTE DOLL -> 536367}  |\n",
      "|{IVORY KNITTED MUG COSY  -> 536367}            |\n",
      "|{BOX OF 6 ASSORTED COLOUR TEASPOONS -> 536367} |\n",
      "|{BOX OF VINTAGE JIGSAW BLOCKS  -> 536367}      |\n",
      "|{BOX OF VINTAGE ALPHABET BLOCKS -> 536367}     |\n",
      "|{HOME BUILDING BLOCK WORD -> 536367}           |\n",
      "|{LOVE BUILDING BLOCK WORD -> 536367}           |\n",
      "|{RECIPE BOX WITH METAL HEART -> 536367}        |\n",
      "+-----------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵 생성 \"\"\"\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex_map: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                          536365|\n",
      "|                          536373|\n",
      "|                          536375|\n",
      "|                          536396|\n",
      "|                          536406|\n",
      "|                          536544|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 데이터 조회 \"\"\"\n",
    "mapped = df \\\n",
    "    .select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "mapped.printSchema()\n",
    "mapped.selectExpr(\"complex_map['WHITE METAL LANTERN']\").where(\"complex_map['WHITE METAL LANTERN'] is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "|CREAM CUPID HEART...|536365|\n",
      "|KNITTED UNION FLA...|536365|\n",
      "|RED WOOLLY HOTTIE...|536365|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 분해 \"\"\"\n",
    "exploded = df \\\n",
    "    .select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .selectExpr(\"explode(complex_map)\")\n",
    "exploded.printSchema()\n",
    "exploded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. JSON 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Json 컬럼 생성 \"\"\"\n",
    "jsonDF = spark.range(1).selectExpr(\n",
    "    \"\"\"\n",
    "    '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|  null|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 인라인 쿼리로 JSON 조회하기 \"\"\"\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"jsonString.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}'),\n",
       " Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}'),\n",
       " Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" StructType을 Json 문자열로 변경 \"\"\"\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\"))) \\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJSON)|             newJSON|\n",
      "+--------------------+--------------------+\n",
      "|{536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|{536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Json 문자열을 객체로 변환 \"\"\"\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) \\\n",
    "    .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")) \\\n",
    "    .show(2) # 키를 컬럼명으로 값을 로우로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>9. [기본]</font> f\"{work_data}/retail-data/by-day/2010-12-01.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 고객구분자(CustomerID)와 설명(Description) 컬럼이 널값인 데이터프레임을 추출하여 출력하세요\n",
    "#### 4. 고객구분자(CustomerID)가 null 인 경우는 0.0 으로 치환하고\n",
    "#### 5. 설명(Description)가 null 인 경우는 \"NOT MENTIONED\" 값으로 저장될 수 있도록 만들어주세요\n",
    "#### 6. 최종 스키마와 데이터를 출력해 주세요\n",
    "\n",
    "<details><summary>[실습9] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df5 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{work_data}/retail-data/by-day/2010-12-01.csv\")\n",
    ").where(expr(\"Description is null or CustomerID is null\"))\n",
    "df5.printSchema()\n",
    "df5.show(10)\n",
    "desc_custid_fill = {\"Description\":\"NOT MENTIONED\", \"CustomerID\":0.0}\n",
    "answer = df5.na.fill(desc_custid_fill)\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|                null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536544|    21773|DECORATIVE ROSE B...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21774|DECORATIVE CATS B...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21786|  POLKADOT RAIN HAT |       4|2010-12-01 14:32:00|     0.85|      null|United Kingdom|\n",
      "|   536544|    21787|RAIN PONCHO RETRO...|       2|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n",
      "|   536544|    21790|  VINTAGE SNAP CARDS|       9|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n",
      "|   536544|    21791|VINTAGE HEADS AND...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21801|CHRISTMAS TREE DE...|      10|2010-12-01 14:32:00|     0.43|      null|United Kingdom|\n",
      "|   536544|    21802|CHRISTMAS TREE HE...|       9|2010-12-01 14:32:00|     0.43|      null|United Kingdom|\n",
      "|   536544|    21803|CHRISTMAS TREE ST...|      11|2010-12-01 14:32:00|     0.43|      null|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = false)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th></tr>\n",
       "<tr><td>536414</td><td>22139</td><td>NOT MENTIONED</td><td>56</td><td>2010-12-01 11:52:00</td><td>0.0</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21773</td><td>DECORATIVE ROSE BATHROOM BOTTLE</td><td>1</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21774</td><td>DECORATIVE CATS BATHROOM BOTTLE</td><td>2</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21786</td><td>POLKADOT RAIN HAT </td><td>4</td><td>2010-12-01 14:32:00</td><td>0.85</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21787</td><td>RAIN PONCHO RETROSPOT</td><td>2</td><td>2010-12-01 14:32:00</td><td>1.66</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21790</td><td>VINTAGE SNAP CARDS</td><td>9</td><td>2010-12-01 14:32:00</td><td>1.66</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21791</td><td>VINTAGE HEADS AND TAILS CARD GAME </td><td>2</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21801</td><td>CHRISTMAS TREE DECORATION WITH BELL</td><td>10</td><td>2010-12-01 14:32:00</td><td>0.43</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21802</td><td>CHRISTMAS TREE HEART DECORATION</td><td>9</td><td>2010-12-01 14:32:00</td><td>0.43</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21803</td><td>CHRISTMAS TREE STAR DECORATION</td><td>11</td><td>2010-12-01 14:32:00</td><td>0.43</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21809</td><td>CHRISTMAS HANGING TREE WITH BELL</td><td>1</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21810</td><td>CHRISTMAS HANGING STAR WITH BELL</td><td>3</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21811</td><td>CHRISTMAS HANGING HEART WITH BELL</td><td>1</td><td>2010-12-01 14:32:00</td><td>2.51</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21821</td><td>GLITTER STAR GARLAND WITH BELLS </td><td>1</td><td>2010-12-01 14:32:00</td><td>7.62</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21822</td><td>GLITTER CHRISTMAS TREE WITH BELLS</td><td>1</td><td>2010-12-01 14:32:00</td><td>4.21</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21823</td><td>PAINTED METAL HEART WITH HOLLY BELL</td><td>2</td><td>2010-12-01 14:32:00</td><td>2.98</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21844</td><td>RED RETROSPOT MUG</td><td>2</td><td>2010-12-01 14:32:00</td><td>5.91</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21851</td><td>LILAC DIAMANTE PEN IN GIFT BOX</td><td>1</td><td>2010-12-01 14:32:00</td><td>4.21</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21870</td><td>I CAN ONLY PLEASE ONE PERSON MUG</td><td>1</td><td>2010-12-01 14:32:00</td><td>3.36</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "<tr><td>536544</td><td>21871</td><td>SAVE THE PLANET MUG</td><td>5</td><td>2010-12-01 14:32:00</td><td>3.36</td><td>0.0</td><td>United Kingdom</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
       "|InvoiceNo|StockCode|                        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
       "|   536414|    22139|                      NOT MENTIONED|      56|2010-12-01 11:52:00|      0.0|       0.0|United Kingdom|\n",
       "|   536544|    21773|    DECORATIVE ROSE BATHROOM BOTTLE|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21774|    DECORATIVE CATS BATHROOM BOTTLE|       2|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21786|                 POLKADOT RAIN HAT |       4|2010-12-01 14:32:00|     0.85|       0.0|United Kingdom|\n",
       "|   536544|    21787|              RAIN PONCHO RETROSPOT|       2|2010-12-01 14:32:00|     1.66|       0.0|United Kingdom|\n",
       "|   536544|    21790|                 VINTAGE SNAP CARDS|       9|2010-12-01 14:32:00|     1.66|       0.0|United Kingdom|\n",
       "|   536544|    21791| VINTAGE HEADS AND TAILS CARD GAME |       2|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21801|CHRISTMAS TREE DECORATION WITH BELL|      10|2010-12-01 14:32:00|     0.43|       0.0|United Kingdom|\n",
       "|   536544|    21802|    CHRISTMAS TREE HEART DECORATION|       9|2010-12-01 14:32:00|     0.43|       0.0|United Kingdom|\n",
       "|   536544|    21803|     CHRISTMAS TREE STAR DECORATION|      11|2010-12-01 14:32:00|     0.43|       0.0|United Kingdom|\n",
       "|   536544|    21809|   CHRISTMAS HANGING TREE WITH BELL|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21810|   CHRISTMAS HANGING STAR WITH BELL|       3|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21811|  CHRISTMAS HANGING HEART WITH BELL|       1|2010-12-01 14:32:00|     2.51|       0.0|United Kingdom|\n",
       "|   536544|    21821|   GLITTER STAR GARLAND WITH BELLS |       1|2010-12-01 14:32:00|     7.62|       0.0|United Kingdom|\n",
       "|   536544|    21822|  GLITTER CHRISTMAS TREE WITH BELLS|       1|2010-12-01 14:32:00|     4.21|       0.0|United Kingdom|\n",
       "|   536544|    21823|PAINTED METAL HEART WITH HOLLY BELL|       2|2010-12-01 14:32:00|     2.98|       0.0|United Kingdom|\n",
       "|   536544|    21844|                  RED RETROSPOT MUG|       2|2010-12-01 14:32:00|     5.91|       0.0|United Kingdom|\n",
       "|   536544|    21851|     LILAC DIAMANTE PEN IN GIFT BOX|       1|2010-12-01 14:32:00|     4.21|       0.0|United Kingdom|\n",
       "|   536544|    21870|   I CAN ONLY PLEASE ONE PERSON MUG|       1|2010-12-01 14:32:00|     3.36|       0.0|United Kingdom|\n",
       "|   536544|    21871|                SAVE THE PLANET MUG|       5|2010-12-01 14:32:00|     3.36|       0.0|United Kingdom|\n",
       "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>\n",
    "#### 4. [PySpark Search](https://spark.apache.org/docs/latest/api/python/search.html)\n",
    "#### 5. [Pyspark Functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#module-pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
