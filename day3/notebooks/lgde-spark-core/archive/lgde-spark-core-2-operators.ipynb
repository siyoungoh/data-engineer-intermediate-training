{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2교시 기본 연산자 다루기\n",
    "\n",
    "> 스파크의 \"기본 연산자\" 와 \"데이터프레임\"에 대해 학습합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 기본 연산자](#1.-기본-연산자)\n",
    "* [2. RDD 의 특징과 데이터 변환의 종류](#3.-RDD-의-특징과-데이터-변환의-종류)\n",
    "* [3. 데이터 타입](#4.-데이터-타입)\n",
    "* [4. 핵심 데이터 프레임 연산자](#2.-핵심-데이터-프레임-연산자)\n",
    "* [5. 기타 데이터 프레임 연산자](#5.-기타-데이터-프레임-연산자)\n",
    "* [6. 데이터셋 API](#6.-데이터셋-API)\n",
    "* [7. 카탈리스트 옵티마이저](#7.-카탈리스트-옵티마이저)\n",
    "* [참고자료](#참고자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/02 15:23:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/09/02 15:23:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c1aa5f4167a8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f426b7d9eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 연산자\n",
    "\n",
    "* RDD(Rsilient Distributed Dataset): 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음\n",
    "  - RDD: 불변성, 한번 생성하면 변경할 수 없음\n",
    "  - Transformation: 원하는 변경 방법을 알려주는 명령. 추상적인 변경 방법이며, 실제 수행X\n",
    "  - Lazy Evaluation(지연 연산): 특정 연산 명령을 하면 즉시 데이터를 수정하지 않고, 원시 데이터에 적용할 트랜스포메이션 실행계획 생성하고, 실제로 연산 그래프를 처리하기 직전까지 기다림\n",
    "\n",
    "---\n",
    "### 1.1 데이터 프레임 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.printSchema() | 스키마 정보를 출력합니다. | - |\n",
    "| df.schema | StructType 스키마를 반환합니다 | - |\n",
    "| df.columns | 컬럼명 정보를 반환합니다 | - |\n",
    "| df.show(n) | 데이터 n 개를 출력합니다 | - |\n",
    "| df.first() | 데이터 프레임의 첫 번째 Row 를 반환합니다 | - |\n",
    "| df.head(n) | 데이터 프레임의 처음부터 n 개의 Row 를 반환합니다 | - |\n",
    "| df.createOrReplaceTempView | 임시 뷰 테이블을 생성합니다 | - |\n",
    "| df.union(newdf) | 데이터프레임 간의 유니온 연산을 수행합니다 | - |\n",
    "| df.limit(n) | 추출할 로우수 제한 | T |\n",
    "| df.repartition(n) | 파티션 재분배, 셔플발생 | - |\n",
    "| df.coalesce() | 셔플하지 않고 파티션을 병합 | 마지막 스테이지의 reduce 수가 줄어드는 효과로 성능저하에 유의해야 합니다 |\n",
    "| df.collect() | 모든 데이터 수집, 반환 | A |\n",
    "| df.take(n) | 상위 n개 로우 반환 | A |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 컬럼 함수\n",
    "\n",
    "#### 컬럼 이란?\n",
    "> <strong>컬럼: 표현식을 사용해 레코드 단위로 계산한 값을 단순하게 나타내는 논리적인 구조</strong> (테이블의 컬럼으로 생각할 수 있음)\n",
    "* 컬럼의 실제값을 얻으려면 로우가 필요하고, 로우를 얻으려면 DataFrame이 필요 (로우는 데이터 레코드)\n",
    "  - DataFrame을 통하지 않으면 외부에서 컬럼에 접근 불가\n",
    "  - DataFrame -> Row -> Column\n",
    "* 컬럼의 특징\n",
    "  - 컬럼의 내용을 수정하려면 반드시 DataFrame의 스파크 트랜포메이션을 사용\n",
    "  - col, column 함수를 사용하는 것이 가장 간편 (책에서는 col 함수 사용)\n",
    "  - 컬럼은 컬럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태\n",
    "  - 분석기가 동작하는 단계에서 컬럼과 테이블을 분석\n",
    "\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.select | 컬럼이나 표현식 사용  | - |\n",
    "| df.selectExpr | 문자열 표현식 사용 = df.select(expr()) | - |\n",
    "| df.withColumn(컬럼명, 표현식) | 컬럼 추가, 비교, 컬럼명 변경 | - |\n",
    "| df.withColumnRenamed(old_name, new_name) | 컬럼명 변경 | - |\n",
    "| df.drop() | 컬럼 삭제 | - |\n",
    "| df.where | 로우 필터링 | - |\n",
    "| df.filter | 로우 필터링 | - |\n",
    "| df.sort, df.orderBy | 정렬 | - |\n",
    "| df.sortWithinPartitions | 파티션별 정렬 | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 기타 함수\n",
    "\n",
    "#### 표현식\n",
    "> <strong>표현식: DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미</strong>\n",
    "\n",
    "* 여러 컬럼명을 입력받아 식별하고 단일 값을 만들기 위해 다양한 표현식을 각 레코드에 적용하는 함수\n",
    "  - <strong>표현식은 expr함수로 사용</strong>\n",
    "  - <strong>컬럼은 단지 표현식일 뿐</strong>\n",
    "  - <strong>표현식은 연산 순서를 지정하는 논리적 트리로 컴파일됨</strong>\n",
    "\n",
    "---\n",
    "\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| expr(\"someCol - 5\") | 표현식 | - |\n",
    "| lit() | 리터럴 | - |\n",
    "| cast() | 컬럼 데이터 타입 변경 | - |\n",
    "| distinct() | unique row | - |\n",
    "| desc(), asc() | 정렬 순서 | - |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Structured API 활용 가이드\n",
    "\n",
    "+ <strong>DataFrame</strong> 구성요소: 레코드, 컬럼\n",
    "    + 레코드: Row타입(Table의 로우)\n",
    "    + 컬럼: 레코드에 수행할 연산 표현식(테이블의 컬럼)\n",
    "+ <strong>스키마</strong>: 각 컬럼명과 데이터 타입을 정의\n",
    "+ <strong>파티셔닝</strong>: DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의\n",
    "+ <strong>파티셔닝 스키마</strong>: 파티션을 구성하는 방법을 정의 (테이블 Root Directory 의 Sub Directories 구성을 말합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDD 의 특징과 데이터 변환의 종류\n",
    "\n",
    "| 특징 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| dependencies | resilency | 리니지를 통해 의존성 정보를 유지함으로써 언제든 다시 수행할 수 있는 회복력을 가집니다 |\n",
    "| partitions | parallelize computation | 파티션 단위로 데이터를 저장 관리하므로써 병렬 처리를 가능하게 합니다 |\n",
    "| compute function | Iterator\\[T\\] | RDD로 저장되는 모든 데이터는 반복자를 통해 함수를 적용할 수 있습니다 |\n",
    "\n",
    "* 반면에 compute function 의 내부를 spark 가 알 수 없기 때문에 오류를 찾아내가 어려우며, Python 과 같은 스크립트 언어는 generic object 로만 인식이 되므로 호환하기 어려우며, T 타입의 객체는 직렬화되어 전달되기만 할 뿐 스파크는 해당 데이터 타입 T 에 대해 알 수 없습니다\n",
    "\n",
    "> RDD 를 통해 데이터 처리하는 방법과, 구조화된 API 를 통해 처리하는 방법을 비교해 보고, 이러한 고수준의 DSL 연산자를 통해 보다 단순하게 표현이 가능합니다.\n",
    "\n",
    "### 2.1 RDD 통한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| Age|\n",
      "+------+----+\n",
      "|   Cat|27.0|\n",
      "|   Dog|19.0|\n",
      "|Monkey|28.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRDD = spark.sparkContext.parallelize([(\"Cat\", 30), (\"Dog\", 28), (\"Monkey\", 28), (\"Cat\", 24), (\"Dog\", 10)])\n",
    "agesRDD = (\n",
    "    dataRDD.map(lambda x: (x[0], (x[1], 1)))\n",
    "     .reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1]))\n",
    "     .map(lambda v: (v[0], v[1][0]/v[1][1]))\n",
    ")\n",
    "agesRDD.toDF([\"Name\", \"Age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 구조화 API 통한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Name  |Age |\n",
      "+------+----+\n",
      "|Monkey|28.0|\n",
      "|Cat   |27.0|\n",
      "|Dog   |19.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"동물의 평균 수명\").getOrCreate()\n",
    "animal = spark.createDataFrame([(\"Cat\", 30), (\"Dog\", 28), (\"Monkey\", 28), (\"Cat\", 24), (\"Dog\", 10)], [\"Name\", \"Age\"])\n",
    "ages = animal.select(\"Name\", \"Age\").groupBy(\"Name\").agg(avg(\"Age\").alias(\"Age\"))\n",
    "ages.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 타입\n",
    "> immutable 하며, 모든 transformation 들의 lineage 를 유지합니다. 또한 컬럼을 변경, 추가 등을 통해 새로운 데이터프레임을 생성합니다.\n",
    "\n",
    "| python | scala |\n",
    "|---|---|\n",
    "| ![python](images/datatypes-python.png) | ![scala](images/datatypes-scala.png) | \n",
    "| ![python](images/datatypes-python2.png) | ![scala](images/datatypes-scala2.png) | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 핵심 데이터 프레임 연산자\n",
    "\n",
    "### 4.1 파일로 부터 테이블 만들어 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\")\n",
    "df = spark.read.json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"2015_summary\")\n",
    "\n",
    "sql_result = spark.sql(\"SELECT * FROM 2015_summary\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 특정 컬럼 선택 (select, selectExpr)\n",
    "> 아래의 모든 예제에서 컬럼 선택 시에 select(col(\"컬럼명\")) 으로 접근할 수도 있지만 **selectExpr(\"컬럼명\") 이 간결하기 때문에 앞으로는 가능한 표현식으로 사용**하겠습니다 <br>\n",
    "컬럼 표현식의 경우 반드시 하나의 컬럼은 하나씩 표현되어야만 합니다.  <br>\n",
    "잘된예 : \"컬럼1\", \"컬럼2\" <br>\n",
    "잘못된예: \"컬럼1, 컬럼2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\")\n",
    "df.select(upper(col(\"DEST_COUNTRY_NAME\")), \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "print(\"# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\")\n",
    "df.selectExpr(\"upper(DEST_COUNTRY_NAME)\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\n",
      "+-------------+-----------------+\n",
      "| newColmnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\")\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColmnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(someCol - 5)'>\n",
      "Column<'(someCol - 5)'>\n",
      "Column<'(someCol - 5)'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column<'((((somecol + 5) * 200) - 6) < othercol)'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "print(expr(\"someCol - 5\"))\n",
    "print(expr(\"someCol\") - 5)\n",
    "print(col(\"someCol\") - 5)\n",
    "\n",
    "# 동일한 트랜스포메이션 과정: 스파크는 연산 순서를 지정하는 논리적 트리로 컴파일합니다\n",
    "(((col(\"somecol\") + 5) * 200) - 6) < col(\"othercol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 논리적 트리로 컴파일되는 표현식 \"\"\"\n",
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dag.png](images/dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> f\"{work_data}/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. selectExpr 구문 혹은 spark sql 구문을 이용하여 DEST_COUNTRY_NAME 컬럼은 대문자로, ORIGIN_COUNTRY_NAME 컬럼을 소문자로 2개의 컬럼을 조회하세요\n",
    "\n",
    "<details><summary>[실습1] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df1.printSchema()\n",
    "answer = df1.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select upper(DEST_COUNTRY_NAME), lower(ORIGIN_COUNTRY_NAME) from 2015_summary\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+------------------------+--------------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|lower(ORIGIN_COUNTRY_NAME)|\n",
      "+------------------------+--------------------------+\n",
      "|           UNITED STATES|                   romania|\n",
      "|           UNITED STATES|                   croatia|\n",
      "|           UNITED STATES|                   ireland|\n",
      "|                   EGYPT|             united states|\n",
      "|           UNITED STATES|                     india|\n",
      "|           UNITED STATES|                 singapore|\n",
      "|           UNITED STATES|                   grenada|\n",
      "|              COSTA RICA|             united states|\n",
      "|                 SENEGAL|             united states|\n",
      "|                 MOLDOVA|             united states|\n",
      "+------------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 상수값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리터럴(literal)을 사용한 리터럴 상수 값 컬럼 추가\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "df.selectExpr(\"*\", \"1 as One\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# withColumn(컬럼명, 표현식) 으로 컬럼 추가\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# withColumn(컬럼명, 표현식) 으로 컬럼 추가\")\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 대소 비교를 통한 불리언 값 반환\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 대소 비교를 통한 불리언 값 반환\")\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\")\n",
    "before = df\n",
    "before.printSchema()\n",
    "\n",
    "after = before.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\"))\n",
    "after.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>2. [기본]</font> f\"{work_data}/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. 기존의 컬럼은 그대로 두고, ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME 2개의 컬럼을 \n",
    "####    각각 소문자, 대문자로 변경한 컬럼 ORIGIN_COUNTRY_NAME_LOWER, DEST_COUNTRY_NAME_UPPER 총 4개의 컬럼을 출력하세요\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df2.printSchema()\n",
    "answer = df2.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"\"\"select \n",
    "    ORIGIN_COUNTRY_NAME, \n",
    "    DEST_COUNTRY_NAME, \n",
    "    lower(ORIGIN_COUNTRY_NAME) as ORIGIN_COUNTRY_NAME_LOWER, \n",
    "    upper(DEST_COUNTRY_NAME) as DEST_COUNTRY_NAME_UPPER \n",
    "    from 2015_summary\"\"\"\n",
    ").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-------------------+-----------------+-------------------------+-----------------------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME_LOWER|DEST_COUNTRY_NAME_UPPER|\n",
      "+-------------------+-----------------+-------------------------+-----------------------+\n",
      "|            Romania|    United States|                  romania|          UNITED STATES|\n",
      "|            Croatia|    United States|                  croatia|          UNITED STATES|\n",
      "|            Ireland|    United States|                  ireland|          UNITED STATES|\n",
      "|      United States|            Egypt|            united states|                  EGYPT|\n",
      "|              India|    United States|                    india|          UNITED STATES|\n",
      "|          Singapore|    United States|                singapore|          UNITED STATES|\n",
      "|            Grenada|    United States|                  grenada|          UNITED STATES|\n",
      "|      United States|       Costa Rica|            united states|             COSTA RICA|\n",
      "|      United States|          Senegal|            united states|                SENEGAL|\n",
      "|      United States|          Moldova|            united states|                MOLDOVA|\n",
      "+-------------------+-----------------+-------------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 컬럼명 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼 명 변경하기\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Destination', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 컬럼 명 변경하기\")\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 특정 컬럼을 제거합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 특정 컬럼을 제거합니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\")\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "caseSensitive = df.drop(\"dest_country_name\")\n",
    "caseSensitive.printSchema()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)\n",
    "caseInsensitive = df.drop(\"dest_country_name\")\n",
    "caseInsensitive.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 한 번에 여러 컬럼도 삭제할 수 있습니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['count']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 한 번에 여러 컬럼도 삭제할 수 있습니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns # 여러 컬럼을 지우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>3. [중급]</font> f\"{work_data}/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. ORIGIN_COUNTRY_NAME 컬럼은 Origin 으로 이름을 변경하고\n",
    "#### 2. DEST_COUNTRY_NAME 컬럼은 DestUpper 으로 대문자로 변경한 컬럼을 추가하고\n",
    "#### 3. DEST_COUNTRY_NAME 컬럼은 Drop 하고, Origin, DestUpper 2개의 컬럼만 남은 DataFrame 의 데이터를 출력하세요\n",
    "#### 4. 최종 스키마를 출력하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df3.printSchema()\n",
    "answer = (df3\n",
    "    .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"Origin\")\n",
    "    .withColumn(\"DestUpper\", upper(\"DEST_COUNTRY_NAME\"))\n",
    "    .drop(\"DEST_COUNTRY_NAME\", \"count\")\n",
    ")\n",
    "answer.show()\n",
    "answer.printSchema()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+----------------+--------------------+\n",
      "|          Origin|           DestUpper|\n",
      "+----------------+--------------------+\n",
      "|         Romania|       UNITED STATES|\n",
      "|         Croatia|       UNITED STATES|\n",
      "|         Ireland|       UNITED STATES|\n",
      "|   United States|               EGYPT|\n",
      "|           India|       UNITED STATES|\n",
      "|       Singapore|       UNITED STATES|\n",
      "|         Grenada|       UNITED STATES|\n",
      "|   United States|          COSTA RICA|\n",
      "|   United States|             SENEGAL|\n",
      "|   United States|             MOLDOVA|\n",
      "|    Sint Maarten|       UNITED STATES|\n",
      "|Marshall Islands|       UNITED STATES|\n",
      "|   United States|              GUYANA|\n",
      "|   United States|               MALTA|\n",
      "|   United States|            ANGUILLA|\n",
      "|   United States|             BOLIVIA|\n",
      "|        Paraguay|       UNITED STATES|\n",
      "|   United States|             ALGERIA|\n",
      "|   United States|TURKS AND CAICOS ...|\n",
      "|       Gibraltar|       UNITED STATES|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- DestUpper: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 컬럼의 데이터 타입 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 데이터 유형을 변경합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|       15|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|      344|\n",
      "|            Egypt|      United States|   15|       15|\n",
      "|    United States|              India|   62|       62|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|int_count|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|    United States|            Romania|   15|       15|       15|\n",
      "|    United States|            Croatia|    1|        1|        1|\n",
      "|    United States|            Ireland|  344|      344|      344|\n",
      "|            Egypt|      United States|   15|       15|       15|\n",
      "|    United States|              India|   62|       62|       62|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      " |-- int_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 데이터 유형을 변경합니다\")\n",
    "df.printSchema()\n",
    "\n",
    "int2str = df.withColumn(\"str_count\", col(\"count\").cast(\"string\"))\n",
    "int2str.show(5)\n",
    "int2str.printSchema()\n",
    "\n",
    "str2int = int2str.withColumn(\"int_count\", col(\"str_count\").cast(\"int\"))\n",
    "str2int.show(5)\n",
    "str2int.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 레코드 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Where 와 Filter 는 동일합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Where 와 Filter 는 동일합니다\")\n",
    "df.where(\"count < 2\").show(2)\n",
    "df.filter(\"count < 2\").show(2)\n",
    "\n",
    "print(\"# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\")\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 유일 값 (DISTINCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "\"\"\" distinct 함수 \"\"\"\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count())\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count())\n",
    "# distinctcount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>4. [기본] </font> f\"{work_data}/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터 10건을 출력하세요\n",
    "#### 3. count 가 5000 이상, 100000 보다 미만인 ORIGIN_COUNTRY_NAME 를 출력하되 중복을 제거해 주세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df4.printSchema()\n",
    "df4.show(10)\n",
    "df4.selectExpr(\"min(count)\", \"max(count)\").show()\n",
    "answer = df4.where(expr(\"count >= 5000 and count < 100000\")).select(\"ORIGIN_COUNTRY_NAME\")\n",
    "answer.distinct()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+----------+\n",
      "|min(count)|max(count)|\n",
      "+----------+----------+\n",
      "|         1|    370002|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ORIGIN_COUNTRY_NAME</th></tr>\n",
       "<tr><td>Mexico</td></tr>\n",
       "<tr><td>Canada</td></tr>\n",
       "<tr><td>United States</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+\n",
       "|ORIGIN_COUNTRY_NAME|\n",
       "+-------------------+\n",
       "|             Mexico|\n",
       "|             Canada|\n",
       "|      United States|\n",
       "+-------------------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 정렬 (SORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sort 와 orderBy 함수는 동일한 효과를 가집니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# sort 와 orderBy 함수는 동일한 효과를 가집니다\")\n",
    "df.sort(\"count\").show(2)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "print(\"# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\")\n",
    "df.sort(\"DEST_COUNTRY_NAME\").show(1)\n",
    "df.sort(df[\"DEST_COUNTRY_NAME\"].asc_nulls_first()).show(1)\n",
    "df.sort(df.DEST_COUNTRY_NAME.asc_nulls_first()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Vietnam|    2|\n",
      "|    United States|          Venezuela|  246|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Angola|   13|\n",
      "|    United States|           Anguilla|   38|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\") \n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(df[\"count\"].desc()).show(2)\n",
    "df.orderBy(df.ORIGIN_COUNTRY_NAME.desc(), df.DEST_COUNTRY_NAME.asc()).show(2)\n",
    "df.orderBy(expr(\"ORIGIN_COUNTRY_NAME DESC\"), expr(\"DEST_COUNTRY_NAME ASC\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 로우 수 제한 (LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12 예약 문자와 키워드\n",
    "\n",
    "> 공백이나 하이픈(-) 같은 예약 문자를 컬럼명에 사용하려면 백틱(`) 문자를 사용해야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------+\n",
      "|This Long Column-Name|\n",
      "+---------------------+\n",
      "|              Romania|\n",
      "|              Croatia|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" withColumn, selectExpr, select 차이점 \"\"\"\n",
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\")) # 첫 번째 인수에서 사용하지 않음\n",
    "dfWithLongColName.show(2)\n",
    "\n",
    "dfWithLongColName.selectExpr(\"`This Long Column-Name`\", \"`This Long Column-Name` as `new col`\").show(2) # 사용함\n",
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.13 임의 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "193\n"
     ]
    }
   ],
   "source": [
    "\"\"\" randomSplit 함수 \"\"\"\n",
    "seed = 42\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "print(dataFrames[0].count())\n",
    "print(dataFrames[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.14 로우 합치기와 추가하기\n",
    "+ 동일한 스키마와 컬럼 수를 가져야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    5|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" union 함수 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "\n",
    "# Parallelized Collections :\n",
    "# Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program.\n",
    "# The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. \n",
    "\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows) \n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union = (\n",
    "    df.union(newDF)\n",
    "    .where(\"count = 1\")\n",
    "    .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>5. [중급]</font> f\"{work_data}/flight-data/json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 2010 ~ 2012년까지 년도별로 별도의 데이터프레임으로 읽되, 년도 컬럼 `year int`를 추가하세요\n",
    "#### 2. 모든 데이터 프레임을 하나의 데이터프레임으로 묶어서 년도별 레코드 수를 출력하세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "flight2010 = spark.read.json(f\"{work_data}/flight-data/json/2010-summary.json\").withColumn(\"year\", lit(2010))\n",
    "flight2011 = spark.read.json(f\"{work_data}/flight-data/json/2011-summary.json\").withColumn(\"year\", lit(2011))\n",
    "flight2012 = spark.read.json(f\"{work_data}/flight-data/json/2012-summary.json\").withColumn(\"year\", lit(2012))\n",
    "\n",
    "flight = flight2010.union(flight2011).union(flight2012)\n",
    "flight.printSchema()\n",
    "flight.groupBy(\"year\").count()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>count</th></tr>\n",
       "<tr><td>2012</td><td>245</td></tr>\n",
       "<tr><td>2011</td><td>255</td></tr>\n",
       "<tr><td>2010</td><td>255</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----+\n",
       "|year|count|\n",
       "+----+-----+\n",
       "|2012|  245|\n",
       "|2011|  255|\n",
       "|2010|  255|\n",
       "+----+-----+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15 repartition과 coalesce\n",
    "> 향후에 사용할 파티션 수가 현재 파티션 수보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에 사용(repartition, 셔플이 필수로 발생)\n",
    "* 자주 필터링되는 컬럼을 기준으로 파티션 재분배를 권장\n",
    "* repartition: 물리적인 데이터 구성 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파티션 나누기 \"\"\"\n",
    "df.rdd.getNumPartitions()\n",
    "repart_1 = df.repartition(5)\n",
    "repart_2 = df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "repart_3 = df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "\n",
    "# repartition 숫자 변경 등 실험내용 공유\n",
    "print(repart_1.rdd.getNumPartitions())\n",
    "print(repart_2.rdd.getNumPartitions())\n",
    "print(repart_3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 셔플하지 않고 파티션을 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr>\n",
       "<tr><td>Moldova</td><td>United States</td><td>1</td></tr>\n",
       "<tr><td>Bolivia</td><td>United States</td><td>30</td></tr>\n",
       "<tr><td>Algeria</td><td>United States</td><td>4</td></tr>\n",
       "<tr><td>Turks and Caicos Islands</td><td>United States</td><td>230</td></tr>\n",
       "<tr><td>Pakistan</td><td>United States</td><td>12</td></tr>\n",
       "<tr><td>Marshall Islands</td><td>United States</td><td>42</td></tr>\n",
       "<tr><td>Suriname</td><td>United States</td><td>1</td></tr>\n",
       "<tr><td>Panama</td><td>United States</td><td>510</td></tr>\n",
       "<tr><td>New Zealand</td><td>United States</td><td>111</td></tr>\n",
       "<tr><td>Liberia</td><td>United States</td><td>2</td></tr>\n",
       "<tr><td>Ireland</td><td>United States</td><td>335</td></tr>\n",
       "<tr><td>Zambia</td><td>United States</td><td>1</td></tr>\n",
       "<tr><td>Malaysia</td><td>United States</td><td>2</td></tr>\n",
       "<tr><td>Japan</td><td>United States</td><td>1548</td></tr>\n",
       "<tr><td>French Polynesia</td><td>United States</td><td>43</td></tr>\n",
       "<tr><td>Singapore</td><td>United States</td><td>3</td></tr>\n",
       "<tr><td>Denmark</td><td>United States</td><td>153</td></tr>\n",
       "<tr><td>Spain</td><td>United States</td><td>420</td></tr>\n",
       "<tr><td>Bermuda</td><td>United States</td><td>183</td></tr>\n",
       "<tr><td>Kiribati</td><td>United States</td><td>26</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------------------+-------------------+-----+\n",
       "|       DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
       "+------------------------+-------------------+-----+\n",
       "|                 Moldova|      United States|    1|\n",
       "|                 Bolivia|      United States|   30|\n",
       "|                 Algeria|      United States|    4|\n",
       "|Turks and Caicos Islands|      United States|  230|\n",
       "|                Pakistan|      United States|   12|\n",
       "|        Marshall Islands|      United States|   42|\n",
       "|                Suriname|      United States|    1|\n",
       "|                  Panama|      United States|  510|\n",
       "|             New Zealand|      United States|  111|\n",
       "|                 Liberia|      United States|    2|\n",
       "|                 Ireland|      United States|  335|\n",
       "|                  Zambia|      United States|    1|\n",
       "|                Malaysia|      United States|    2|\n",
       "|                   Japan|      United States| 1548|\n",
       "|        French Polynesia|      United States|   43|\n",
       "|               Singapore|      United States|    3|\n",
       "|                 Denmark|      United States|  153|\n",
       "|                   Spain|      United States|  420|\n",
       "|                 Bermuda|      United States|  183|\n",
       "|                Kiribati|      United States|   26|\n",
       "+------------------------+-------------------+-----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 파티션 합치기 \"\"\"\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.16 드라이버로 로우 데이터 수집하기\n",
    "> 대규모 데이터셋에 collect 명령을 수행하면 드라이버 비정상 종료 우려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(5)\n",
    "collectDF.take(5) # 정수를 인수값으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show()  # 결과를 정돈된 형태로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.collect() # 전체 모든 테이터를 수집, 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7f14fc013660>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 대규모 데이터셋에 collect, toLocalIterator 수행하면 매우 큰 비용(cpu, 메모리, 네트워크) 발생, 드라이버 비정상적 종료 가능성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 기타 데이터 프레임 연산자\n",
    "\n",
    "### 5.1 사전에 스키마를 정의하는 장점\n",
    "* 데이터 타입을 추론에 대한 신경을 쓸 필요가 없다\n",
    "* 스키마 추론을 위한 별도의 작업에 드는 리소스를 줄일 수 있다\n",
    "* 스키마에 맞지 않는 데이터의 오류를 빠르게 인지할 수 있다\n",
    "\n",
    "### 5.2 스키마를 정의하는 두 가지 방법\n",
    "* 1. 프로그래밍 방식으로 정의하는 방법\n",
    "  - 스키마: 여러 개의 StructField 타입 필드로 구성된 StructType 객체\n",
    "  - [StructField](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/types/StructField.html): (이름), (데이터 타입), (컬럼이 값이 없거나 null일 수 있는지 지정하는 불리언 값) 으로 구성\n",
    "    - StructField(String name, DataType dataType, boolean nullable, Metadata metadata)\n",
    "  - Metadata: 해당 컬럼과 관련된 정보이며, 스파크의 머신러닝 라이브러리에서 사용\n",
    "* 2. DDL 구문을 이용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1. Programming Style\n",
      "StructType(List(StructField(author,StringType,false),StructField(title,StringType,false),StructField(pages,IntegerType,false)))\n",
      "root\n",
      " |-- author: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- pages: integer (nullable = false)\n",
      "\n",
      "+----------+----------------------------+-----+\n",
      "|author    |title                       |pages|\n",
      "+----------+----------------------------+-----+\n",
      "|정휘센    |안녕하세요 정휘센 입니다    |300  |\n",
      "|김싸이언  |안녕하세요 김싸이언 입니다  |200  |\n",
      "|유코드제로|안녕하세요 유코드제로 입니다|100  |\n",
      "+----------+----------------------------+-----+\n",
      "\n",
      "\n",
      "# 2. DDL Style\n",
      "`author` string, `title` string, `pages` int\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      "\n",
      "+----------+----------------------------+-----+\n",
      "|author    |title                       |pages|\n",
      "+----------+----------------------------+-----+\n",
      "|정휘센    |안녕하세요 정휘센 입니다    |300  |\n",
      "|김싸이언  |안녕하세요 김싸이언 입니다  |200  |\n",
      "|유코드제로|안녕하세요 유코드제로 입니다|100  |\n",
      "+----------+----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    [\"정휘센\", \"안녕하세요 정휘센 입니다\", 300],\n",
    "    [\"김싸이언\", \"안녕하세요 김싸이언 입니다\", 200],\n",
    "    [\"유코드제로\", \"안녕하세요 유코드제로 입니다\", 100]\n",
    "]\n",
    "\n",
    "print(\"# 1. Programming Style\")\n",
    "schema1 = StructType([\n",
    "    StructField(\"author\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"pages\", IntegerType(), False),\n",
    "])\n",
    "print(schema1)\n",
    "df1 = spark.createDataFrame(data, schema1)\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "rows = [\n",
    "    Row(\"정휘센\", \"안녕하세요 정휘센 입니다\", 300),\n",
    "    Row(\"김싸이언\", \"안녕하세요 김싸이언 입니다\", 200),\n",
    "    Row(\"유코드제로\", \"안녕하세요 유코드제로 입니다\", 100)\n",
    "]\n",
    "\n",
    "print(\"\\n# 2. DDL Style\")\n",
    "schema2 = \"`author` string, `title` string, `pages` int\"\n",
    "print(schema2)\n",
    "df2 = spark.createDataFrame(rows, schema2)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "assert(df1.subtract(df2).count() == 0)\n",
    "assert(df2.subtract(df1).count() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>6. [중급]</font> Row 와 문자열을 통한 스키마 구현을 통해 데이터 프레임을 생성하세요\n",
    "#### 1. 스키마 : id int, name string, payment int\n",
    "#### 2. 아래의 데이터 결과가 출력이 되도록 데이터를 생성하세요\n",
    "\n",
    "| id | name | payment |\n",
    "| --- | --- | --- |\n",
    "| 1 | 엘지전자 | 1000 |\n",
    "| 2 | 엘지화학 | 2000 |\n",
    "| 3 | 엘지디스플레이 | 3000 |\n",
    "\n",
    "#### 3. 스키마를 출력하세요\n",
    "#### 4. 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습6] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df6 = [\n",
    "    Row(1, \"엘지전자\", 1000),\n",
    "    Row(2, \"엘지화학\", 2000),\n",
    "    Row(3, \"엘지디스플레이\", 3000)\n",
    "]\n",
    "sc6 = \"`id` int, `name` string, `payment` int\"\n",
    "answer = spark.createDataFrame(df6, sc6)\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th><th>payment</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>엘지디스플레이</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------+-------+\n",
       "| id|          name|payment|\n",
       "+---+--------------+-------+\n",
       "|  1|      엘지전자|   1000|\n",
       "|  2|      엘지화학|   2000|\n",
       "|  3|엘지디스플레이|   3000|\n",
       "+---+--------------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 중첩된 배열 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "|Id |First|Last |Url              |Published|Hits|Campaigns          |\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "|1  |Jules|Damji|https://tinyurl.1|1/4/2016 |4535|[twitter, LinkedIn]|\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), False),\n",
    "    StructField(\"First\", StringType(), False),\n",
    "    StructField(\"Last\", StringType(), False),\n",
    "    StructField(\"Url\", StringType(), False),\n",
    "    StructField(\"Published\", StringType(), False),\n",
    "    StructField(\"Hits\", IntegerType(), False),\n",
    "    StructField(\"Campaigns\", ArrayType(StringType()), False),\n",
    "])\n",
    "path=f\"{work_data}/learning-spark/blogs.json\"\n",
    "blogDF = spark.read.schema(schema).json(path)\n",
    "blogDF.printSchema()\n",
    "blogDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 컬럼과 표현식\n",
    "> 컬럼은 공용 메소드들을 가진 객체들이며, pyspark.sql.functions.expr() 함수를 이용하여 표현식을 그대로 사용할 수 있습니다\n",
    "\n",
    "* 특히 컬럼 함수를 통해 다양한 연산자를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import Column\n",
    "print(blogDF.columns)\n",
    "# help(Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      AuthorsId|\n",
      "+---------------+\n",
      "|  Jules.Damji@1|\n",
      "| Brooke.Wenig@2|\n",
      "|    Denny.Lee@3|\n",
      "|Tathagata.Das@4|\n",
      "+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.withColumn(\"AuthorsId\", (concat(expr(\"First\"), lit(\".\"), expr(\"Last\"), lit(\"@\"), expr(\"Id\")))) \\\n",
    ".select(col(\"AuthorsId\")) \\\n",
    ".show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.select(expr(\"Hits\")).show(2)\n",
    "blogDF.select(col(\"Hits\")).show(2)\n",
    "blogDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.sort(col(\"Id\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 로우 생성 및 다루기\n",
    "> 로우의 경우 컬럼을 인덱스를 기준으로 접근할 수 있습니다.\n",
    "\n",
    "#### 레코드와 로우\n",
    "+ 스파크는 레코드를 Row 객체로 표현\n",
    "  - (각 로우는 하나의 레코드, '로우'와 '레코드'를 같은 의미로 사용)\n",
    "  - (대문자로 시작하는 Row는 Row 객체를 의미)\n",
    "+ Row 객체는 내부에 바이트 배열을 가지며, 오직 컬럼 표현식으로만 다룰 수 있으므로 사용자에게 노출되지 않음\n",
    "+ DataFrame을 사용해 드라이버에게 개별 로우를 반환하는 명령은 항상 하나 이상의 Row 타입을 반환\n",
    "+ Row 객체는 스키마 정보를 가지고 있지 않음 (DataFrame만 유일하게 스키마를 가지고 있음)\n",
    "+ Row 객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Row를 확인하는 예문 \"\"\"\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reynold\n",
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    "[\"twitter\", \"LinkedIn\"])\n",
    "print(blog_row[1])\n",
    "\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "|CallNumber|UnitID|IncidentNumber|CallType        |CallDate  |RowID        |\n",
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "|20110016  |T13   |2003235       |Structure Fire  |01/11/2002|020110016-T13|\n",
      "|20110022  |M17   |2003241       |Medical Incident|01/11/2002|020110022-M17|\n",
      "|20110023  |M41   |2003242       |Medical Incident|01/11/2002|020110023-M41|\n",
      "|20110032  |E11   |2003250       |Vehicle Fire    |01/11/2002|020110032-E11|\n",
      "|20110043  |B04   |2003259       |Alarms          |01/11/2002|020110043-B04|\n",
      "|20110072  |T08   |2003279       |Structure Fire  |01/11/2002|020110072-T08|\n",
      "|20110125  |E33   |2003301       |Alarms          |01/11/2002|020110125-E33|\n",
      "|20110130  |E36   |2003304       |Alarms          |01/11/2002|020110130-E36|\n",
      "|20110197  |E05   |2003343       |Medical Incident|01/11/2002|020110197-E05|\n",
      "|20110215  |E06   |2003348       |Medical Incident|01/11/2002|020110215-E06|\n",
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "StructField('UnitID', StringType(), True),\n",
    "StructField('IncidentNumber', IntegerType(), True),\n",
    "StructField('CallType', StringType(), True),\n",
    "StructField('CallDate', StringType(), True),\n",
    "StructField('WatchDate', StringType(), True),\n",
    "StructField('CallFinalDisposition', StringType(), True),\n",
    "StructField('AvailableDtTm', StringType(), True),\n",
    "StructField('Address', StringType(), True),\n",
    "StructField('City', StringType(), True),\n",
    "StructField('Zipcode', IntegerType(), True),\n",
    "StructField('Battalion', StringType(), True),\n",
    "StructField('StationArea', StringType(), True),\n",
    "StructField('Box', StringType(), True),\n",
    "StructField('OriginalPriority', StringType(), True),\n",
    "StructField('Priority', StringType(), True),\n",
    "StructField('FinalPriority', IntegerType(), True),\n",
    "StructField('ALSUnit', BooleanType(), True),\n",
    "StructField('CallTypeGroup', StringType(), True),\n",
    "StructField('NumAlarms', IntegerType(), True),\n",
    "StructField('UnitType', StringType(), True),\n",
    "StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "StructField('FirePreventionDistrict', StringType(), True),\n",
    "StructField('SupervisorDistrict', StringType(), True),\n",
    "StructField('Neighborhood', StringType(), True),\n",
    "StructField('Location', StringType(), True),\n",
    "StructField('RowID', StringType(), True),\n",
    "StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = f\"{work_data}/learning-spark/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "fire_df.select(\"CallNumber\", \"UnitID\", \"IncidentNumber\", \"CallType\", \"CallDate\", \"RowID\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 파케이 파일 혹은 테이블 저장\n",
    "> save 저장 시에는 해당 경로에 파케이 파일이 저장되고, [saveAsTable](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html) 저장 시에는 테이블로 관리됩니다. 여기서 관리되는 테이블의 경우, Hive Metastore 와 연동되는 것을 가정하고 있지만, 연결되어 있지 않아도 Derby 를 통해 로컬 저장소에 (\"spark.sql.warehouse.dir\" 의 위치) 관리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5.6.1 Parquet File 관리\n",
    "\n",
    "> 테이블이 아니라 별도의 파케이 파일로 저장하고, 하이브의 External 테이블로 관리하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/31 15:48:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetPath=\"target/sf_fire_calls\"\n",
    "fire_df.write.format(\"parquet\").mode(\"overwrite\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 5.6.2 Unmanaged Table 관리는 별도로 경로를 지정합니다 \n",
    "\n",
    "> Overwrite 가능한 External Table 형태로 생성 및 관리됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/31 15:58:24 WARN HadoopFSUtils: The directory file:/home/jovyan/work/lgde-spark-core/spark-warehouse/target/sf_fire_calls_unmanaged was not found. Was it deleted very recently?\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='sf_fire_calls', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sf_fire_calls_unmanaged', database='default', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !rm -rf \"target/sf_fire_calls_unmanaged\"\n",
    "fire_df.repartition(2).write.mode(\"overwrite\").option(\"path\", \"target/sf_fire_calls_unmanaged\").saveAsTable(\"sf_fire_calls_unmanaged\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 5.6.3 Managed Table 관리는 테이블으로만 관리합니다\n",
    "\n",
    "> Managed Table 형태로 생성 및 관리되며, Hive Metastore 연동 시에만 Persistent 한 저장소에 관리되고 Overwrite 되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='sf_fire_calls', database='default', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !rm -rf \"spark-warehouse/sf_fire_calls\"\n",
    "parquetTable=\"sf_fire_calls\"\n",
    "fire_df.write.format(\"parquet\").saveAsTable(parquetTable)\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table default.sf_fire_calls\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 프로젝션과 필터\n",
    "> *Projection*은 특정 관계형 조건 혹은 필터에 매칭되는 로우에 대해서만 반환하는 것을 말합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+-----------------------------+\n",
      "|IncidentNumber|         AvailableDtTm|                     CallType|\n",
      "+--------------+----------------------+-----------------------------+\n",
      "|       2003235|01/11/2002 01:51:44 AM|               Structure Fire|\n",
      "|       2003250|01/11/2002 04:16:46 AM|                 Vehicle Fire|\n",
      "|       2003259|01/11/2002 06:01:58 AM|                       Alarms|\n",
      "|       2003279|01/11/2002 08:03:26 AM|               Structure Fire|\n",
      "|       2003301|01/11/2002 09:46:44 AM|                       Alarms|\n",
      "|       2003304|01/11/2002 09:58:53 AM|                       Alarms|\n",
      "|       2003382|01/11/2002 02:59:04 PM|               Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:08 PM|               Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:08 PM|               Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:08 PM|               Structure Fire|\n",
      "|       2003429|01/11/2002 05:17:15 PM|     Odor (Strange / Unknown)|\n",
      "|       2003453|01/11/2002 06:48:01 PM|                       Alarms|\n",
      "|       2003497|01/11/2002 09:03:17 PM|               Structure Fire|\n",
      "|       2003554|01/12/2002 01:56:32 AM|               Structure Fire|\n",
      "|       2003618|01/12/2002 11:07:36 AM|     Odor (Strange / Unknown)|\n",
      "|       2003649|01/12/2002 01:03:10 PM|     Odor (Strange / Unknown)|\n",
      "|       2003695|01/12/2002 04:46:59 PM|               Structure Fire|\n",
      "|       2003756|01/12/2002 07:54:42 PM|                       Alarms|\n",
      "|       2003770|01/12/2002 08:44:01 PM|Smoke Investigation (Outside)|\n",
      "|       2003777|01/12/2002 09:14:13 PM|               Structure Fire|\n",
      "+--------------+----------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_fire_df = (\n",
    "    fire_df\n",
    "    .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "    .where(col(\"CallType\") != \"Medical Incident\")\n",
    ")\n",
    "print(few_fire_df)\n",
    "few_fire_df.show(5, truncate=False)\n",
    "\n",
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(\n",
    "    new_fire_df\n",
    "    .select(\"ResponseDelayedinMins\")\n",
    "    .where(col(\"ResponseDelayedinMins\") > 5)\n",
    "    .show(5, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 날짜 관련 함수\n",
    "* 날짜의 경우 문자열로 전달되고 있기 때문에 표현 및 활용을 위해서는 to_timestamp(), to_date() 와 같은 날짜관련 함수를 사용할 수 있습니다.\n",
    "  - 한번 timestamp 형태로 변경된 컬럼에 대해서는 year, month, dayofmonth 와 같은 일자관련 함수를 통해 다양한 예제를 실습할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:===================>                                     (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------------+\n",
      "|year(IncidentDate)|month(IncidentDate)|dayofmonth(IncidentDate)|\n",
      "+------------------+-------------------+------------------------+\n",
      "|              2000|                  4|                      12|\n",
      "|              2000|                  4|                      13|\n",
      "|              2000|                  4|                      14|\n",
      "|              2000|                  4|                      15|\n",
      "|              2000|                  4|                      16|\n",
      "+------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fire_ts_df = (new_fire_df\n",
    "    .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\")\n",
    "    .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .drop(\"AvailableDtTm\")\n",
    ")\n",
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "(\n",
    "    fire_ts_df\n",
    "    .select(year('IncidentDate'), month('IncidentDate'), dayofmonth(\"IncidentDate\"))\n",
    "    .distinct()\n",
    "    .orderBy(year('IncidentDate'), month('IncidentDate'), dayofmonth(\"IncidentDate\"))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>7. [기본]</font> '실습6' 에서 생성한 데이터를 \"target/lgde_user\" 경로에 파케이 파일로 저장하세요\n",
    "#### 1. 저장된 데이터를 다시 읽어서 스키마와 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습7] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df6 = [\n",
    "    Row(1, \"엘지전자\", 1000),\n",
    "    Row(2, \"엘지화학\", 2000),\n",
    "    Row(3, \"엘지디스플레이\", 3000)\n",
    "]\n",
    "sc6 = \"`id` int, `name` string, `payment` int\"\n",
    "df7 = spark.createDataFrame(df6, sc6)\n",
    "df7.write.mode(\"overwrite\").parquet(\"target/lgde_user\")\n",
    "\n",
    "answer = spark.read.parquet(\"target/lgde_user\")\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th><th>payment</th></tr>\n",
       "<tr><td>3</td><td>엘지디스플레이</td><td>3000</td></tr>\n",
       "<tr><td>1</td><td>엘지전자</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td><td>2000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------+-------+\n",
       "| id|          name|payment|\n",
       "+---+--------------+-------+\n",
       "|  3|엘지디스플레이|   3000|\n",
       "|  1|      엘지전자|   1000|\n",
       "|  2|      엘지화학|   2000|\n",
       "+---+--------------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터셋 API\n",
    "> Python 과 R 은 compile-time type-safe 하지 않기 때문에, Datasets 통한 Typed 데이터 타입을 사용할 수 없습니다. Datasets 을 이용하는 경우에도 Spark SQL 엔진이 객체를 생성, 변환, 직렬화, 역직렬화를 수행하며, **Dataframe 의 경우와 마찬가지로 Off-heap 을 통한 메모리 관리를 수행**하게 되며, Dataset encoders 를 이용합니다\n",
    "\n",
    "### 6.1 데이터셋과 데이터프레임 비교\n",
    "\n",
    "| Structured APIs | SQL vs. Dataframe vs. Datasets |\n",
    "|---|---|\n",
    "| ![structured-api](images/structured-api.png) | ![sql-vs-dataframes-vs-datasets-type-safety-spectrum](images/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png) |\n",
    "\n",
    "* 언어별 타입 객체 비교\n",
    "![typed-untyped](images/typed-untyped.png)\n",
    "\n",
    "* Scala: Case Class 를 통해 선언\n",
    "```scala\n",
    "case class DeviceIoTData (\n",
    "    battery_level: Long, \n",
    "    c02_level: Long,\n",
    "    cca2: String, \n",
    "    cca3: String, \n",
    "    cn: String, \n",
    "    device_id: Long,\n",
    "    device_name: String, \n",
    "    humidity: Long, \n",
    "    ip: String, \n",
    "    latitude: Double,\n",
    "    lcd: String, \n",
    "    longitude: Double, \n",
    "    scale:String, \n",
    "    temp: Long,\n",
    "    timestamp: Long)\n",
    "```\n",
    "\n",
    "* 데이터를 읽고 DeviceIoTData 클래스로 변환을 수행합니다\n",
    "```scala\n",
    "val ds = spark.read.json(\"/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\").as[DeviceIoTData]\n",
    "val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70}})\n",
    "```\n",
    "* Datasets 이용 시에는 filter(), map(), groupBy(), select(), take() 등의 일반적인 함수를 사용합니다\n",
    "```scala                              \n",
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long, cca3: String)\n",
    "val dsTemp = ds.filter(d => {d.temp > 25})\n",
    "    .map(d => (d.temp, d.device_name, d.device_id, d.cca3))\n",
    "    .toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    "    .as[DeviceTempByCountry]\n",
    "```\n",
    "\n",
    "### 6.2 데이터셋 데이터프레임 그리고 RDD\n",
    "* Datasets\n",
    "  - compile-time 의 type safety 가 필요한 경우\n",
    "* Dataframe\n",
    "  - SQL-like 쿼리를 이용하고자 하는 경우\n",
    "  - 통합, 코드 최적화 그리고 API를 활용한 모듈화를 원하는 경우\n",
    "  - R 혹은 Python 을 이용해야 하는 경우\n",
    "  - 공간, 속도 효율성을 고려해야 하는 경우\n",
    "* RDD\n",
    "  - 별도의 RDD를 이용하는 써드파티 패키지를 사용하는 경우\n",
    "  - 코드, 공간, 속도 최적화 등을 원하지 않는 경우\n",
    "  - 스파크가 수행할 쿼리를 정확히 지시해야만 할 때\n",
    "\n",
    "\n",
    "* RDD와 데이터프레임과 데이터셋은 서로 다른가?\n",
    "  - 데이터프레임과 데이터셋은 RDD 위에서 구현됩니다. 즉, whole-stage code generation 단계에서 압축된 RDD 코드로 분해됩니다.\n",
    "\n",
    "> DataFrames and Datasets are\n",
    "built on top of RDDs, and they get decomposed to compact RDD code during wholestage\n",
    "code generation, which we discuss in the next section\n",
    "\n",
    "* Spark SQL\n",
    "![spark-sql](images/spark-sql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 데이터프레임\n",
    "\n",
    "![transform](images/transform.png)\n",
    "\n",
    "* 데이터프레임을 다루는 연산자의 특징\n",
    "  - 로우나 컬럼 추가\n",
    "  - 로우나 컬럼 제거\n",
    "  - 로우를 컬럼으로 변환하거나, 그 반대로 변환\n",
    "  - 컬럼값을 기준으로 로우 순서 변경\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr>\n",
       "<tr><td>United States</td><td>Romania</td><td>15</td></tr>\n",
       "<tr><td>United States</td><td>Croatia</td><td>1</td></tr>\n",
       "<tr><td>United States</td><td>Ireland</td><td>344</td></tr>\n",
       "<tr><td>Egypt</td><td>United States</td><td>15</td></tr>\n",
       "<tr><td>United States</td><td>India</td><td>62</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------------+-----+\n",
       "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
       "+-----------------+-------------------+-----+\n",
       "|    United States|            Romania|   15|\n",
       "|    United States|            Croatia|    1|\n",
       "|    United States|            Ireland|  344|\n",
       "|            Egypt|      United States|   15|\n",
       "|    United States|              India|   62|\n",
       "+-----------------+-------------------+-----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" 원시 데이터소스 활용 \"\"\"\n",
    "flightData2015 = spark.read.format(\"json\").load(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    "flightData2015.createOrReplaceTempView(\"2015_summary\")\n",
    "sql_result = spark.sql(\"SELECT * FROM 2015_summary LIMIT 5\")\n",
    "display(sql_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 카탈리스트 옵티마이저\n",
    "> Spark SQL 엔진의 핵심이며 크게 4가지 단계로 구분됩니다. \n",
    "\n",
    "### 7.1 분석 (Analysis)\n",
    "* \"추상화 구문 트리(AST, Abstract Syntax Tree)\" 생성 단계로, 모든 테이블명과 컬럼명은 내부적으로 컬럼명, 데이터유형, 함수와 더불어 데이터베이스와 테이블 이름까지 모두 관리하고 있는 *Catalog*에 의해 해석되어 트리 형태의 구조로 생성됩니다\n",
    "\n",
    "### 7.2 논리 최적화 (Logical Optimization)\n",
    "* 카탈리스트 옵티마이저는 우선 다수의 논리적 계획을 세우고, \"비용 기반 옵티마이저(CBO, Cost-Based Optimizer)\"를 이용하여 각 계획에 비용(Cost)를 할당합니다. 이러한 계획은 아래의 \"Figure 3-5\"와 같은 연산자 트리 형태로 구성되며, 이때에 **constant folding, predicate pushdown, projection pruning, Boolean expression simplification** 등의 최적화가 이루어집니다\n",
    "\n",
    "### 7.3 물리 계획 (Physical Planning)\n",
    "* Spark SQL 엔진은 CBO에 의해 선택된 논리 계획에 대해 스파크 엔진에 존재하는 적절한 연산자들을 이용하여 최적의 계획을 생성합니다\n",
    "\n",
    "### 7.4 코드 생성 (Code Generation)\n",
    "* 마지막 단계에서는  Project Tungsten 의 whole-stage code generation 을 통해 [마치 컴파일러와 같이 동작](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)하며, 메모리 상에 로딩된 데이터 집합에 대해 수행될 최적의 자바 바이트 코드를 생성해 냅니다.\n",
    "\n",
    "* What is ***whole-stage code generation***?\n",
    "  - 물리적인 쿼리 최적화 단계를 말하며, 쿼리 전체를 하나의 함수로 만들어 냅니다\n",
    "  - virtual function call 을 제거하거나, 중간 데이터를 CPU registers 에 올리는 등의 최적화 작업을 수행합니다\n",
    "  - Spark 2.0 텅스텐 엔진은 압축된 RDD 코드를 생성하는 방식으로 개선 되었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'GlobalLimit 5\n",
      "+- 'LocalLimit 5\n",
      "   +- 'Project [*]\n",
      "      +- 'UnresolvedRelation [2015_summary], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Project [DEST_COUNTRY_NAME#434, ORIGIN_COUNTRY_NAME#435, count#436L]\n",
      "      +- SubqueryAlias 2015_summary\n",
      "         +- Relation[DEST_COUNTRY_NAME#434,ORIGIN_COUNTRY_NAME#435,count#436L] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Relation[DEST_COUNTRY_NAME#434,ORIGIN_COUNTRY_NAME#435,count#436L] json\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 5\n",
      "+- FileScan json [DEST_COUNTRY_NAME#434,ORIGIN_COUNTRY_NAME#435,count#436L] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/home/jovyan/work/data/flight-data/json/2015-summary.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>8. [중급]</font> f\"{work_data}/databricks/mnm_dataset.csv\" M&M 초콜렛 판매 데이터 CSV 파일을 읽고,\n",
    "#### 1. 초콜릿판매 지역(State), 초콜릿 색깔(Color) 별 총 판매 수 (Total)를 구하되, Spark SQL 이용하여 상위 5개를 출력하세요\n",
    "#### 2. 동일한 결과를 Structured API를 활용하여 작성 후, 동일하게 상위 5개를 출력하세요\n",
    "#### 3. 1~2번의 실행 데이터프레임을 explain(True) 함수를 통해 출력하고 동일한지 여부를 확인해 보세요\n",
    "\n",
    "<details><summary>[실습9] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "\n",
    "mnm_df = spark.read.csv(f\"{work_data}/databricks/mnm_dataset.csv\", inferSchema=True, header=True)\n",
    "mnm_df.createOrReplaceTempView(\"mnm_df\")\n",
    "\n",
    "mnmSql = mnm_df.select(\"State\", \"Color\", \"Count\").groupBy(\"State\", \"Color\").agg(sum(\"Count\").alias(\"Total\")).orderBy(desc(col(\"Total\"))).limit(5)\n",
    "display(mnmSql)\n",
    "mnmSql.explain(True)\n",
    "\n",
    "mnmApi = spark.sql(\"select State, Color, sum(Count) as Total from mnm_df group by State, Color order by Total desc limit 5\")\n",
    "display(mnmApi)\n",
    "mnmApi.explain(True)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>State</th><th>Color</th><th>Total</th></tr>\n",
       "<tr><td>CA</td><td>Yellow</td><td>100956</td></tr>\n",
       "<tr><td>WA</td><td>Green</td><td>96486</td></tr>\n",
       "<tr><td>CA</td><td>Brown</td><td>95762</td></tr>\n",
       "<tr><td>TX</td><td>Green</td><td>95753</td></tr>\n",
       "<tr><td>TX</td><td>Red</td><td>95404</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+------+------+\n",
       "|State| Color| Total|\n",
       "+-----+------+------+\n",
       "|   CA|Yellow|100956|\n",
       "|   WA| Green| 96486|\n",
       "|   CA| Brown| 95762|\n",
       "|   TX| Green| 95753|\n",
       "|   TX|   Red| 95404|\n",
       "+-----+------+------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [Total#203L DESC NULLS LAST], true\n",
      "      +- Aggregate [State#190, Color#191], [State#190, Color#191, sum(cast(Count#192 as bigint)) AS Total#203L]\n",
      "         +- Project [State#190, Color#191, Count#192]\n",
      "            +- Relation[State#190,Color#191,Count#192] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Total: bigint\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [Total#203L DESC NULLS LAST], true\n",
      "      +- Aggregate [State#190, Color#191], [State#190, Color#191, sum(cast(Count#192 as bigint)) AS Total#203L]\n",
      "         +- Project [State#190, Color#191, Count#192]\n",
      "            +- Relation[State#190,Color#191,Count#192] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [Total#203L DESC NULLS LAST], true\n",
      "      +- Aggregate [State#190, Color#191], [State#190, Color#191, sum(cast(Count#192 as bigint)) AS Total#203L]\n",
      "         +- Relation[State#190,Color#191,Count#192] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[Total#203L DESC NULLS LAST], output=[State#190,Color#191,Total#203L])\n",
      "+- *(2) HashAggregate(keys=[State#190, Color#191], functions=[sum(cast(Count#192 as bigint))], output=[State#190, Color#191, Total#203L])\n",
      "   +- Exchange hashpartitioning(State#190, Color#191, 5), ENSURE_REQUIREMENTS, [id=#394]\n",
      "      +- *(1) HashAggregate(keys=[State#190, Color#191], functions=[partial_sum(cast(Count#192 as bigint))], output=[State#190, Color#191, sum#217L])\n",
      "         +- FileScan csv [State#190,Color#191,Count#192] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/databricks/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>State</th><th>Color</th><th>Total</th></tr>\n",
       "<tr><td>CA</td><td>Yellow</td><td>100956</td></tr>\n",
       "<tr><td>WA</td><td>Green</td><td>96486</td></tr>\n",
       "<tr><td>CA</td><td>Brown</td><td>95762</td></tr>\n",
       "<tr><td>TX</td><td>Green</td><td>95753</td></tr>\n",
       "<tr><td>TX</td><td>Red</td><td>95404</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+------+------+\n",
       "|State| Color| Total|\n",
       "+-----+------+------+\n",
       "|   CA|Yellow|100956|\n",
       "|   WA| Green| 96486|\n",
       "|   CA| Brown| 95762|\n",
       "|   TX| Green| 95753|\n",
       "|   TX|   Red| 95404|\n",
       "+-----+------+------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'GlobalLimit 5\n",
      "+- 'LocalLimit 5\n",
      "   +- 'Sort ['Total DESC NULLS LAST], true\n",
      "      +- 'Aggregate ['State, 'Color], ['State, 'Color, 'sum('Count) AS Total#241]\n",
      "         +- 'UnresolvedRelation [mnm_df], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Total: bigint\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [Total#241L DESC NULLS LAST], true\n",
      "      +- Aggregate [State#190, Color#191], [State#190, Color#191, sum(cast(Count#192 as bigint)) AS Total#241L]\n",
      "         +- SubqueryAlias mnm_df\n",
      "            +- Relation[State#190,Color#191,Count#192] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [Total#241L DESC NULLS LAST], true\n",
      "      +- Aggregate [State#190, Color#191], [State#190, Color#191, sum(cast(Count#192 as bigint)) AS Total#241L]\n",
      "         +- Relation[State#190,Color#191,Count#192] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[Total#241L DESC NULLS LAST], output=[State#190,Color#191,Total#241L])\n",
      "+- *(2) HashAggregate(keys=[State#190, Color#191], functions=[sum(cast(Count#192 as bigint))], output=[State#190, Color#191, Total#241L])\n",
      "   +- Exchange hashpartitioning(State#190, Color#191, 5), ENSURE_REQUIREMENTS, [id=#484]\n",
      "      +- *(1) HashAggregate(keys=[State#190, Color#191], functions=[partial_sum(cast(Count#192 as bigint))], output=[State#190, Color#191, sum#257L])\n",
      "         +- FileScan csv [State#190,Color#191,Count#192] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/databricks/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래와 같이 2개의 테이블에 대해 조인, 필터, 프로젝션 등의 연산 시에 아래와 같은 최적화로 **Disk 및 Network I/O 를 줄일 수 있습니다**.\n",
    "  - Predicate Pushdown : 데이터 소스를 모두 읽지 않고, 필터 조건에 해당하는 데이터만 읽습니다\n",
    "  - Column Pruning : 데이터 소스에서 모든 필드를 읽지 않고, 필요한 필터만 읽습니다\n",
    "\n",
    "```scala\n",
    "val users = spark.read.parquet(\"/users/parquet/path\")\n",
    "val events = spark.read.parquet(\"/events/parquet/path\")\n",
    "val joinedDF = users.join(events, users(\"id\") === events(\"uid\"))\n",
    ".filter(events(\"date\") > \"2015-01-01\")\n",
    "```\n",
    "\n",
    "![query-transformation](images/query-transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>9. [중급]</font> f\"{work_data}/tbl_user.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 가장 최근에 가입한 5명을 출력하세요\n",
    "* Structured API를 활용하여 작성하세요\n",
    "* 가입일자 컬럼(u_signup)을 이용하세요\n",
    "\n",
    "<details><summary>[실습9] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "\n",
    "# Spark SQL Style\n",
    "df9 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{work_data}/tbl_user.csv\")\n",
    ")\n",
    "df9.printSchema()\n",
    "df9.show(10)\n",
    "df9.createOrReplaceTempView(\"tbl_user\")\n",
    "answer = spark.sql(\"select * from tbl_user order by u_signup desc limit 5\")\n",
    "display(answer)\n",
    "    \n",
    "# Structured API Style\n",
    "df9 = spark.read.csv(f\"{work_data}/tbl_user.csv\", header=True, inferSchema=True)\n",
    "df9.orderBy(asc(\"u_signup\")).limit(5)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>u_id</th><th>u_name</th><th>u_gender</th><th>u_signup</th></tr>\n",
       "<tr><td>1</td><td>정휘센</td><td>남</td><td>19700808</td></tr>\n",
       "<tr><td>2</td><td>김싸이언</td><td>남</td><td>19710201</td></tr>\n",
       "<tr><td>4</td><td>청소기</td><td>남</td><td>19770329</td></tr>\n",
       "<tr><td>3</td><td>박트롬</td><td>여</td><td>19951030</td></tr>\n",
       "<tr><td>5</td><td>유코드제로</td><td>여</td><td>20021029</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+----------+--------+--------+\n",
       "|u_id|    u_name|u_gender|u_signup|\n",
       "+----+----------+--------+--------+\n",
       "|   1|    정휘센|      남|19700808|\n",
       "|   2|  김싸이언|      남|19710201|\n",
       "|   4|    청소기|      남|19770329|\n",
       "|   3|    박트롬|      여|19951030|\n",
       "|   5|유코드제로|      여|20021029|\n",
       "+----+----------+--------+--------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>10. [중급]</font> f\"{work_data}/tbl_purchase.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 200만원 이상 금액 상품 가운데 상위 3개를 출력하세요\n",
    "* Structured API를 활용하여 작성하세요\n",
    "* 상품가격 컬럼(p_amount)을 이용하세요\n",
    "* 최대한 짧은 구문으로 작성해 보세요\n",
    "\n",
    "<details><summary>[실습10] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "# SQL\n",
    "df10 = spark.read.csv(f\"{work_data}/tbl_purchase.csv\", inferSchema=True, header=True)\n",
    "df10.createOrReplaceTempView(\"tbl_purchase\")\n",
    "answer = spark.sql(\"select * from tbl_purchase where p_amount > 2000000 order by p_amount desc limit 3\")\n",
    "display(answer)\n",
    "\n",
    "# API\n",
    "spark.read.csv(f\"{work_data}/tbl_purchase.csv\", inferSchema=True, header=True).orderBy(desc(\"p_amount\")).limit(3)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>p_time</th><th>p_uid</th><th>p_id</th><th>p_name</th><th>p_amount</th></tr>\n",
       "<tr><td>1603674500</td><td>4</td><td>2004</td><td>LG Computer</td><td>4500000</td></tr>\n",
       "<tr><td>1603665955</td><td>5</td><td>2001</td><td>LG Gram</td><td>3500000</td></tr>\n",
       "<tr><td>1603666155</td><td>5</td><td>2003</td><td>LG TV</td><td>2500000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+----+-----------+--------+\n",
       "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
       "+----------+-----+----+-----------+--------+\n",
       "|1603674500|    4|2004|LG Computer| 4500000|\n",
       "|1603665955|    5|2001|    LG Gram| 3500000|\n",
       "|1603666155|    5|2003|      LG TV| 2500000|\n",
       "+----------+-----+----+-----------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>p_time</th><th>p_uid</th><th>p_id</th><th>p_name</th><th>p_amount</th></tr>\n",
       "<tr><td>1603674500</td><td>4</td><td>2004</td><td>LG Computer</td><td>4500000</td></tr>\n",
       "<tr><td>1603665955</td><td>5</td><td>2001</td><td>LG Gram</td><td>3500000</td></tr>\n",
       "<tr><td>1603666155</td><td>5</td><td>2003</td><td>LG TV</td><td>2500000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+----+-----------+--------+\n",
       "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
       "+----------+-----+----+-----------+--------+\n",
       "|1603674500|    4|2004|LG Computer| 4500000|\n",
       "|1603665955|    5|2001|    LG Gram| 3500000|\n",
       "|1603666155|    5|2003|      LG TV| 2500000|\n",
       "+----------+-----+----+-----------+--------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.1.2/api/sql/\" target=\"_blank\">PySpark 3.1.2 Builtin Functions</a>\n",
    "#### 4. [3-ways-to-create-tables-with-apache-spark](https://towardsdatascience.com/3-ways-to-create-tables-with-apache-spark-32aed0f355ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
